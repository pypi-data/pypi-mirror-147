{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import ipyparallel\n",
    "client = ipyparallel.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "from __future__ import division # python2 \n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "def get_antropogenic_release(xt, c1, c2, r1, r2, w1):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xt : float\n",
    "         polution in lake at time t\n",
    "    c1 : float\n",
    "         center rbf 1\n",
    "    c2 : float\n",
    "         center rbf 2\n",
    "    r1 : float\n",
    "         ratius rbf 1\n",
    "    r2 : float\n",
    "         ratius rbf 2\n",
    "    w1 : float\n",
    "         weight of rbf 1\n",
    "         \n",
    "    note:: w2 = 1 - w1\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    rule = w1*(abs(xt-c1/r1))**3+(1-w1)*(abs(xt-c2/r2))**3\n",
    "    at = min(max(rule, 0.01), 0.1)\n",
    "    return at\n",
    "\n",
    "def lake_model(b=0.42, q=2.0, mean=0.02, stdev=0.001, alpha=0.4,     \n",
    "                 delta=0.98, c1=0.25, c2=0.25, r1=0.5, r2=0.5,\n",
    "                 w1=0.5, nsamples=100, steps=100, seed=None):    \n",
    "    '''runs the lake model for 1 stochastic realisation using specified \n",
    "    random seed.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    b : float\n",
    "        decay rate for P in lake (0.42 = irreversible)\n",
    "    q : float\n",
    "        recycling exponent\n",
    "    mean : float\n",
    "            mean of natural inflows\n",
    "    stdev : float\n",
    "            standard deviation of natural inflows\n",
    "    alpha : float\n",
    "            utility from pollution\n",
    "    delta : float\n",
    "            future utility discount rate\n",
    "    c1 : float\n",
    "    c2 : float\n",
    "    r1 : float\n",
    "    r2 : float\n",
    "    w1 : float\n",
    "    steps : int\n",
    "            the number of time steps (e.g., days)\n",
    "    seed : int, optional\n",
    "           seed for the random number generator\n",
    "    \n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    Pcrit = brentq(lambda x: x**q/(1+x**q) - b*x, 0.01, 1.5)\n",
    "    X = np.zeros((steps,))\n",
    "    decisions = np.zeros((steps,))\n",
    "\n",
    "    X[0] = 0.0\n",
    "\n",
    "    natural_inflows = np.random.lognormal(\n",
    "            math.log(mean**2 / math.sqrt(stdev**2 + mean**2)),\n",
    "            math.sqrt(math.log(1.0 + stdev**2 / mean**2)),\n",
    "            size=steps)\n",
    "\n",
    "    for t in range(steps-1):\n",
    "        decisions[t] = get_antropogenic_release(X[t], c1, c2, r1, r2, w1)\n",
    "        X[t+1] = (1-b)*X[t] + X[t]**q/(1+X[t]**q) + decisions[t] + natural_inflows[t]\n",
    "\n",
    "    reliability = np.sum(X < Pcrit)/steps\n",
    "    utility = np.sum(alpha*decisions*np.power(delta,np.arange(steps)))\n",
    "    \n",
    "    # note that I have slightly changed this formulation to retain\n",
    "    # consistency with the equations in the papers\n",
    "    inertia = np.sum(np.abs(np.diff(decisions)) < 0.01)/(steps-1)\n",
    "    return X, utility, inertia, reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open exploration with the Exploratory Modelling Workbench\n",
    "\n",
    "In this blog, I will continue to showcase the functionality of the exploratory modelling. In the [previous blog](https://wordpress.com/post/waterprogramming.wordpress.com/14714), I have given a general introduction to the workbench, and showed how the DPS example that comes with Rhodium can be adapted for use with the workbench. In this blogpost, I will showcase how the workbench can be used for open exploration. \n",
    "\n",
    "## first a short background\n",
    "In exploratory modeling, we are interested in understanding how regions in the uncertainty space and/or the decision lever space map to the outcome space, or partitions thereof. There are two general approaches for investigating this mapping. The first one is through systematic sampling of the uncertainty or lever space. This is sometimes also known as open exploration. The second one is to search through the space in a directed manner using some type of optimization approach. This is sometimes also known as directed search. \n",
    "\n",
    "The workbench support both open exploration and directed search. Both can be applied to investigate the mapping of the uncertainty space or the decision lever space to the outcome space. In most applications, search is used for finding promising mappings from the lever space to the outcome space, while exploration is used to stress test these mappings under a whole range of possible resolutions to the various uncertainties. This need not be the case however. Optimization can be used to discover the worst possible scenario, while sampling can be used to get insight into the sensitivity of outcomes to the various decision levers. \n",
    "\n",
    "## open exploration\n",
    "\n",
    "To showcase the functionality, let's start with a simple example. We are going to simultaneously sample over uncertainties and levers. We are going to generate 100 scenarios, 10 policies, and see how they jointly affect the outcomes. By default, the workbench will use Latin Hypercube sampling for generating both the scenarios and the policies. Each policy will be always evaluated over all scenarios (i.e. a full factorial over scenarios and policies). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "def process_p(values):\n",
    "    values = np.asarray(values)\n",
    "    values = np.mean(values, axis=0)\n",
    "    return np.max(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench import (RealParameter, ScalarOutcome, Constant, \n",
    "                           ReplicatorModel)\n",
    "\n",
    "model = ReplicatorModel('lakeproblem', function=lake_model)\n",
    "model.replications = 150\n",
    "\n",
    "#specify uncertainties\n",
    "model.uncertainties = [RealParameter('b', 0.1, 0.45),\n",
    "                       RealParameter('q', 2.0, 4.5),\n",
    "                       RealParameter('mean', 0.01, 0.05),\n",
    "                       RealParameter('stdev', 0.001, 0.005),\n",
    "                       RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "# set levers\n",
    "model.levers = [RealParameter(\"c1\", -2, 2),\n",
    "                RealParameter(\"c2\", -2, 2),\n",
    "                RealParameter(\"r1\", 0, 2), \n",
    "                RealParameter(\"r2\", 0, 2), \n",
    "                RealParameter(\"w1\", 0, 1)]\n",
    "\n",
    "\n",
    "\n",
    "#specify outcomes \n",
    "model.outcomes = [ScalarOutcome('max_P', kind=ScalarOutcome.MINIMIZE,\n",
    "                                function=process_p),\n",
    "                  ScalarOutcome('utility', kind=ScalarOutcome.MAXIMIZE,\n",
    "                                function=np.mean),\n",
    "                  ScalarOutcome('inertia', kind=ScalarOutcome.MINIMIZE,\n",
    "                                function=np.mean),\n",
    "                  ScalarOutcome('reliability', kind=ScalarOutcome.MAXIMIZE,\n",
    "                                function=np.mean)]\n",
    "\n",
    "# override some of the defaults of the model\n",
    "model.constants = [Constant('alpha', 0.41),\n",
    "                   Constant('steps', 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] performing experiments using ipyparallel\n",
      "[MainProcess/INFO] performing 100 scenarios * 10 policies * 1 model(s) = 1000 experiments\n",
      "[MainProcess/INFO] 100 cases completed\n",
      "[MainProcess/INFO] 200 cases completed\n",
      "[MainProcess/INFO] 300 cases completed\n",
      "[MainProcess/INFO] 400 cases completed\n",
      "[MainProcess/INFO] 500 cases completed\n",
      "[MainProcess/INFO] 600 cases completed\n",
      "[MainProcess/INFO] 700 cases completed\n",
      "[MainProcess/INFO] 800 cases completed\n",
      "[MainProcess/INFO] 900 cases completed\n",
      "[MainProcess/INFO] 1000 cases completed\n",
      "[MainProcess/INFO] experiments finished\n"
     ]
    }
   ],
   "source": [
    "from ema_workbench import (IpyparallelEvaluator, ema_logging, \n",
    "                           perform_experiments)\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with IpyparallelEvaluator(model, client) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=100, policies=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "experiments, outcomes = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import pairs_plotting\n",
    "\n",
    "fig, axes = pairs_plotting.pairs_scatter(results, group_by='policy', \n",
    "                                         legend=False)\n",
    "fig.set_size_inches(8,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### advanced analysis\n",
    "\n",
    "* scenario discovery\n",
    "* feature scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import prim\n",
    "\n",
    "x = experiments\n",
    "y = outcomes['max_P'] <0.8\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8)\n",
    "box1 = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpld3\n",
    "box1.show_tradeoff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "for key, value in outcomes.items():\n",
    "    params = model.uncertainties #+ model.levers[:]\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=len(params), sharey=True)\n",
    "    y = value\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "        ax = axes[i]\n",
    "        ax.set_xlabel(param.name)\n",
    "        \n",
    "        pearson = sp.stats.pearsonr(x, y)\n",
    "        ax.annotate(\"r: {:6.3f}\".format(pearson[0]), xy=(0.15, 0.85), \n",
    "                    xycoords='axes fraction',fontsize=13)\n",
    "        \n",
    "        x = experiments[param.name]\n",
    "        sns.regplot(x, y, ax=ax, ci=None, color='k',\n",
    "                    scatter_kws={'alpha':0.2, 's':8, 'color':'gray'})\n",
    "        ax.set_xlim(param.lower_bound, param.upper_bound)\n",
    "        \n",
    "        \n",
    "    axes[0].set_ylabel(key)    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import feature_scoring\n",
    "\n",
    "x = experiments\n",
    "y = outcomes\n",
    "\n",
    "fs = feature_scoring.get_feature_scores_all(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(fs, cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More advanced sampling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    sa_results = evaluator.perform_experiments(scenarios=1000, \n",
    "                                               uncertainty_sampling='sobol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from SALib.analyze import sobol\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "\n",
    "experiments, outcomes = sa_results\n",
    "\n",
    "problem = get_SALib_problem(model.uncertainties)\n",
    "Si = sobol.analyze(problem, outcomes['max_P'], \n",
    "                   calc_second_order=True, print_to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si_filter = {k:Si[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "Si_df = pd.DataFrame(Si_filter, index=problem['names'])\n",
    "Si_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
