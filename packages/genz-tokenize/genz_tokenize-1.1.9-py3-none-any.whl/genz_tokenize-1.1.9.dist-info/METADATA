Metadata-Version: 2.1
Name: genz-tokenize
Version: 1.1.9
Summary: Vietnamese tokenization, preprocess and models NLP
Home-page: https://github.com/nghiemIUH/genz-tokenize
Author: Van Nghiem
Author-email: vannghiem848@gmail.com
License: MIT
Keywords: nlp task
Platform: UNKNOWN
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: Keras-Preprocessing (==1.1.2)
Requires-Dist: Markdown (==3.3.6)
Requires-Dist: PyYAML (==6.0)
Requires-Dist: Pygments (==2.11.2)
Requires-Dist: SecretStorage (==3.3.1)
Requires-Dist: Werkzeug (==2.0.3)
Requires-Dist: absl-py (==1.0.0)
Requires-Dist: astunparse (==1.6.3)
Requires-Dist: autopep8 (==1.6.0)
Requires-Dist: bleach (==4.1.0)
Requires-Dist: build (==0.7.0)
Requires-Dist: cachetools (==5.0.0)
Requires-Dist: certifi (==2021.10.8)
Requires-Dist: cffi (==1.15.0)
Requires-Dist: charset-normalizer (==2.0.12)
Requires-Dist: click (==8.0.4)
Requires-Dist: colorama (==0.4.4)
Requires-Dist: cryptography (==36.0.1)
Requires-Dist: docutils (==0.18.1)
Requires-Dist: filelock (==3.6.0)
Requires-Dist: flatbuffers (==2.0)
Requires-Dist: gast (==0.5.3)
Requires-Dist: google-auth-oauthlib (==0.4.6)
Requires-Dist: google-auth (==2.6.2)
Requires-Dist: google-pasta (==0.2.0)
Requires-Dist: grpcio (==1.44.0)
Requires-Dist: h5py (==3.6.0)
Requires-Dist: huggingface-hub (==0.4.0)
Requires-Dist: idna (==3.3)
Requires-Dist: importlib-metadata (==4.11.2)
Requires-Dist: jeepney (==0.7.1)
Requires-Dist: joblib (==1.1.0)
Requires-Dist: keras (==2.8.0)
Requires-Dist: keyring (==23.5.0)
Requires-Dist: libclang (==13.0.0)
Requires-Dist: numpy (==1.22.2)
Requires-Dist: oauthlib (==3.2.0)
Requires-Dist: opt-einsum (==3.3.0)
Requires-Dist: packaging (==21.3)
Requires-Dist: pep517 (==0.12.0)
Requires-Dist: pkginfo (==1.8.2)
Requires-Dist: protobuf (==3.19.4)
Requires-Dist: pyasn1-modules (==0.2.8)
Requires-Dist: pyasn1 (==0.4.8)
Requires-Dist: pycodestyle (==2.8.0)
Requires-Dist: pycparser (==2.21)
Requires-Dist: pyparsing (==3.0.7)
Requires-Dist: readme-renderer (==32.0)
Requires-Dist: regex (==2022.3.2)
Requires-Dist: requests-oauthlib (==1.3.1)
Requires-Dist: requests-toolbelt (==0.9.1)
Requires-Dist: requests (==2.27.1)
Requires-Dist: rfc3986 (==2.0.0)
Requires-Dist: rsa (==4.8)
Requires-Dist: sacremoses (==0.0.47)
Requires-Dist: six (==1.16.0)
Requires-Dist: tensorboard-data-server (==0.6.1)
Requires-Dist: tensorboard-plugin-wit (==1.8.1)
Requires-Dist: tensorboard (==2.8.0)
Requires-Dist: tensorflow-io-gcs-filesystem (==0.24.0)
Requires-Dist: tensorflow (==2.8.0)
Requires-Dist: termcolor (==1.1.0)
Requires-Dist: tf-estimator-nightly (==2.8.0.dev2021122109)
Requires-Dist: tokenizers (==0.11.6)
Requires-Dist: toml (==0.10.2)
Requires-Dist: tomli (==2.0.1)
Requires-Dist: tqdm (==4.63.0)
Requires-Dist: transformers (==4.17.0)
Requires-Dist: twine (==3.8.0)
Requires-Dist: typing-extensions (==4.1.1)
Requires-Dist: urllib3 (==1.26.8)
Requires-Dist: webencodings (==0.5.1)
Requires-Dist: wrapt (==1.14.0)
Requires-Dist: zipp (==3.7.0)

# Genz Tokenize

## Installation:

    pip install genz-tokenize

## Using for tokenize basic

```python
    >>> from genz_tokenize import Tokenize
    # using vocab from lib
    >>> tokenize = Tokenize()
    >>> print(tokenize('sinh_viên công_nghệ', 'hello', max_len = 10, padding = True, truncation = True))
    # {'input_ids': [1, 770, 1444, 2, 2, 30469, 2, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'sequence_id': [None, 0, 0, None, None, 1, None]}

    >>> print(tokenize.decode([1, 770, 2]))
    # <s> sinh_viên </s>

    # from your vocab
    >>> tokenize = Tokenize.fromFile('vocab.txt','bpe.codes')
```

## Using bert tokenize inheritance from PreTrainedTokenizer [transformers](https://github.com/huggingface/transformers)

```python
    >>> from genz_tokenize import TokenizeForBert
    # Using vocab from lib
    >>> tokenize = TokenizeForBert()
    >>> print(tokenize(['sinh_viên công_nghệ', 'hello'], max_length=5, padding='max_length',truncation=True))
    # {'input_ids': [[1, 770, 1444, 2, 0], [1, 30469, 2, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0], [1, 1, 1, 0, 0]]}

    # Using your vocab
    >>> tokenize = TokenizeForBert.fromFile('vocab.txt','bpe.codes')
```

## Embedding matrix from fasttext

```python
    >>> from genz_tokenize import get_embedding_matrix
    >>> embedding_matrix = get_embedding_matrix()
```

## Model

    1. Seq2Seq with Bahdanau Attention
    2. Transformer classification
    3. Transformer

## Trainer

```python
    >>> from genz_tokenize.models.utils import Config
    >>> from genz_tokenize.models import Seq2Seq, Transformer, TransformerClassification
    >>> from genz_tokenize.models.training import TrainArgument, Trainer
    # create config hyper parameter
    >>> config = Config()
    >>> config.vocab_size = 100
    >>> config.target_vocab_size = 120
    >>> config.units = 16
    >>> config.maxlen = 20
    # initial model
    >>> model = Seq2Seq(config)
    >>> x = tf.zeros(shape=(10, config.maxlen))
    >>> y = tf.zeros(shape=(10, config.maxlen))
    # create dataset
    >>> BUFFER_SIZE = len(x)
    >>> dataset_train = tf.data.Dataset.from_tensor_slices((x, y))
    >>> dataset_train = dataset_train.shuffle(BUFFER_SIZE)
    >>> dataset_train = dataset_train.batch(2)
    >>> dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)

    >>> args = TrainArgument(batch_size=2, epochs=2)
    >>> trainer = Trainer(model=model, args=args, data_train=dataset_train)
    >>> trainer.train()
```

### [Create your vocab](https://github.com/rsennrich/subword-nmt)


