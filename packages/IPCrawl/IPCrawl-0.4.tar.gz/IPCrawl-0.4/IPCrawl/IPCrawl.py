import requests
import random
import time
import json
from lxml import etree
from redis import Redis
from bs4 import BeautifulSoup
import copy

#构建请求头
headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
        }
# 测试ip的URL
url_for_test ='http://httpbin.org/ip'

def ip_test(self, ip_info,host,port,password):
    '''
    测试爬取到的ip，测试成功则存入Redis
    '''
    if type(ip_info) == list:
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies = {
                'http': ip_for_test,  # 'http://'+
            }
            try:
                response = requests.get(url_for_test, headers=headers, proxies=proxies, timeout=5)
                if response.status_code == 200:
                    print('测试通过:', proxies)
                    x = write_to_Redis(host,port,password,ip_for_test)
                    if x == 1:
                        n += 1
            except Exception as e:
                print('测试失败:', proxies)
                continue
        print('本次向数据库更新数目:', n)
    elif type(ip_info) == str:
        proxies = {
            'http': ip_info,  # 'http://'+
        }
        try:
            response = requests.get(url_for_test, headers=headers, proxies=proxies, timeout=5)
            if response.status_code == 200:
                print('测试通过:', proxies)
                x = self.write_to_Redis(host,port,password,ip_info)
                if x == 1:
                    n += 1
        except Exception as e:
            print('测试失败:', proxies)
    else:
        print('输入类型错误')

def write_to_Redis(host,port,password,proxies):
    '''
    将测试通过的ip存入Redis
    '''
    conn = Redis(host=host, port=port, password=password)
    ex = self.conn.sadd('Proxies', proxies)
    if ex == 1:
        print('Proxies更新成功')
        return 1
    else:
        print('已存在，未更新。')
        return 0

def get_random_ip(host,port,password):
    '''
    随机取出一个ip
    '''
    host = input('请输入redis服务器地址:')
    x = input('请输入密码:')
    conn = Redis(host=host, port=port, password=password)
    useful_proxy_list_bytes = list(conn.smembers('Proxies'))
    useful_proxy_list = []
    for bytes in useful_proxy_list_bytes:
        bytes = bytes.decode('utf-8')
        useful_proxy_list.append(bytes)
    print(useful_proxy_list)
    useful_proxy = random.choice(useful_proxy_list)
    proxy = {
        'http': str(useful_proxy),
    }
    try:
        response = requests.get(url_for_test, headers=headers, proxies=proxy, timeout=5)
        if response.status_code == 200:
            print('此ip未失效:', useful_proxy)
            return useful_proxy
    except Exception as e:
        print('此ip已失效:', useful_proxy)
        conn.srem('Proxies', useful_proxy)
        print('已经从Redis移除')
        get_random_ip()

#IP66代理
class Ip66Proxy():
    def __init__(self):
        # 爬取代理的URL地址，选择的是66代理
        self.url_ip = 'http://www.66ip.cn/index.html'
    def crawl_ip66_ip(self,num):
        '''
        爬取代理ip地址
        '''
        ip_list =[]
        url = self.url_ip
        response = requests.get(url,headers=headers,) #,proxies = proxies需要时可设置代理
        response.encoding = 'gbk'
        soup = BeautifulSoup(response.text, 'lxml')
        soup = soup.find('div', attrs={'class': 'container'}).find('table')
        for item in soup.find_all('tr')[1:]:
            if item.find_all('td')[0].text.strip() != 'ip':
                ipv4 = item.find_all('td')[0].text.strip()
                port = item.find_all('td')[1].text.strip()
                ip = ipv4 + ':' + port
                ip_list.append(copy.deepcopy(ip))
        ip_set = set(ip_list)  # 去掉可能重复的ip
        ip_list = list(ip_set)
        print(ip_list)
        return ip_list

    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                print(response.status_code)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
                else:
                    print('测试失败:', proxies)
                time.sleep(5)
            except Exception as e:
                print('测试失败:',proxies,e)
                pass
        print('本次向数据库更新数目:',n)

    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self,host,password):
        self.conn = Redis(host=host,port=6379,password=password)
        #爬取代理ip1页
        ip_info = self.crawl_ip66_ip(1)
        #测试ip是否可用并存储至redis
        self.ip_test(ip_info)

#Fatezero代理
class FatezeroIP():
    def __init__(self):
        self.url_ip = 'http://proxylist.fatezero.org/proxy.list'

    def crawl_fatezero_ip(self,num):
        '''
        爬取代理ip地址
        '''
        ip_list =[]
        url = self.url_ip
        response = requests.get(url,headers=headers,) #,proxies = proxies需要时可设置代理
        dic_list = response.text.split('\n')
        for item in dic_list:
            if not item.strip(): continue
            item = json.loads(item)
            ipv4 = item['host']
            port = str(item['port'])
            ip = ipv4 + ':' + port
            ip_list.append(copy.deepcopy(ip))
            print(ip)
        ip_set = set(ip_list)  # 去掉可能重复的ip
        ip_list = list(ip_set)
        print(ip_list)
        return ip_list
    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                print(response.status_code)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
                else:
                    print('测试失败:', proxies)
                time.sleep(5)
            except Exception as e:
                print('测试失败:',proxies,e)
                pass
        print('本次向数据库更新数目:',n)
    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self,host,password):
        self.conn = Redis(host=host, port=6379, password=password)
        #爬取代理ip1页
        ip_info = self.crawl_fatezero_ip(1)
        #测试ip是否可用并存储至redis
        self.ip_test(ip_info)

#IP3366代理/云代理
class Ip3366():
    def __init__(self):
        # 爬取代理的URL地址，选择的是3366代理
        self.url_ip ="http://www.ip3366.net/free/?page="

    def crawl_3366_ip(self,num):
        '''
        爬取代理ip地址
        '''
        ip_list =[]
        for num_page in range(1,num+1):
            url = self.url_ip + str(num_page)
            response = requests.get(url,headers=headers,) #,proxies = proxies需要时可设置代理
            print(response)
            if response.status_code ==200:
                content = response.text
                tree = etree.HTML(content)
                tr_list = tree.xpath('/html/body/div[2]/div/div[2]/table/tbody/tr')
                for tr in tr_list:
                    ipv4 = tr.xpath('./td[1]/text()')[0]
                    port = tr.xpath('./td[2]/text()')[0]
                    ip = ipv4 + ':' + port
                    ip_list.append(copy.deepcopy(ip))
            print(ip_list)
            #不睡封ip
            time.sleep(5)
        ip_set = set(ip_list)  # 去掉可能重复的ip
        ip_list = list(ip_set)
        print(ip_list)
        return ip_list
    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            print(n)
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
            except Exception as e:
                print('测试失败:',proxies)
                continue
        print('本次向数据库更新数目:',n)
    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self,host,password):
        self.conn = Redis(host=host, port=6379, password=password)
        #爬取代理ip5页
        ip_info = self.crawl_3366_ip(1)
        #测试ip是否可用并存储至redis
        self.ip_test(ip_info)

#Jiangxianli代理
class JiangxianliIP():
    def __init__(self):
        # 爬取代理的URL地址，选择的是快代理
        self.url_ip = 'https://ip.jiangxianli.com/?anonymity=2&page='

    def crawl_jiangxianli_ip(self,num):
        '''
        爬取代理ip地址
        '''
        ip_list =[]
        for page_num in range(1,num+1):
            url = self.url_ip + str(page_num)
            response = requests.get(url,headers=headers,) #,proxies = proxies需要时可设置代理
            if response.status_code == 200:
                content = response.text
                tree = etree.HTML(content)
                tr_list = tree.xpath('/html/body/div/div[2]/div/div/table/tbody/tr')
                for tr in tr_list:
                    try:
                        ipv4 = tr.xpath('./td[1]/text()')[0]
                        port = tr.xpath('./td[2]/text()')[0]
                        ip = ipv4 + ':' + port
                        ip_list.append(copy.deepcopy(ip))
                        print(ip)
                    except:
                        pass
        ip_set = set(ip_list)  # 去掉可能重复的ip
        ip_list = list(ip_set)
        print(ip_list)
        return ip_list
    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                print(response.status_code)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
                else:
                    print('测试失败:', proxies)
                time.sleep(5)
            except Exception as e:
                print('测试失败:',proxies,e)
                pass
        print('本次向数据库更新数目:',n)
    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self,host,password):
        self.conn = Redis(host=host, port=6379, password=password)
        #爬取代理ip2页
        ip_info = self.crawl_jiangxianli_ip(2)
        #测试ip是否可用并存储至redis
        self.ip_test(ip_info)

#快代理
class KuaidailiProxy():
    def __init__(self):
        # 爬取代理的URL地址，选择的是快代理
        self.url_ip ="https://www.kuaidaili.com/free/inha/"

    def crawl_kuai_ip(self,num):
        '''
        爬取代理ip地址
        '''
        ip_list =[]
        for num_page in range(1,num+1):
            url = self.url_ip + str(num_page)
            response = requests.get(url,headers=headers,) #,proxies = proxies需要时可设置代理
            print(response)
            if response.status_code ==200:
                content = response.text
                tree = etree.HTML(content)
                tr_list = tree.xpath('/html/body/div[1]/div[4]/div[2]/div[2]/div[2]/table/tbody/tr')
                for tr in tr_list:
                    ipv4 = tr.xpath('./td[1]/text()')[0]
                    port = tr.xpath('./td[2]/text()')[0]
                    ip = ipv4 + ':' + port
                    ip_list.append(copy.deepcopy(ip))
            print(ip_list)
            #不睡封ip
            time.sleep(5)
        ip_set = set(ip_list)  # 去掉可能重复的ip
        ip_list = list(ip_set)
        print(ip_list)
        return ip_list
    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
            except Exception as e:
                print('测试失败:',proxies)
                continue
        print('本次向数据库更新数目:',n)
    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self,host,password):
        self.conn = Redis(host=host, port=6379, password=password)
        #爬取代理ip5页
        ip_info = self.crawl_kuai_ip(4)
        #测试ip是否可用并存储至redis
        self.ip_test(ip_info)

#Seofangfa方法代理
class SeofangfaProxy():
    def __init__(self):
        self.url_ip = 'https://proxy.seofangfa.com/'

    def crawl_seofangfa_ip(self,num):
        '''
        爬取代理ip地址
        '''
        ip_list =[]
        url = self.url_ip
        response = requests.get(url,headers=headers,) #,proxies = proxies需要时可设置代理
        tree = etree.HTML(response.text)
        tr_list = tree.xpath('//tbody/tr')
        for tr in tr_list:
            ipv4 = tr.xpath('./td[1]/text()')[0]
            port = tr.xpath('./td[2]/text()')[0]
            ip = ipv4 + ':' + port
            ip_list.append(copy.deepcopy(ip))
            print(ip)
        ip_set = set(ip_list)  # 去掉可能重复的ip
        ip_list = list(ip_set)
        print(ip_list)
        return ip_list
    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                print(response.status_code)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
                else:
                    print('测试失败:', proxies)
                time.sleep(5)
            except Exception as e:
                print('测试失败:',proxies,e)
                pass
        print('本次向数据库更新数目:',n)
    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self,host,password):
        self.conn = Redis(host=host, port=6379, password=password)
        #爬取代理ip1页
        ip_info = self.crawl_seofangfa_ip(1)
        #测试ip是否可用并存储至redis
        self.ip_test(ip_info)

#太阳代理
class TaiyangProxy():
    def __init__(self):
        # 爬取代理的URL地址，选择的是taiyang代理
        self.url_ip ="http://www.taiyanghttp.com/free/page"

    def crawl_taiyang_ip(self,num):
        '''
        爬取代理ip地址
        '''
        ip_list =[]
        for num_page in range(1,num+1):
            url = self.url_ip + str(num_page)
            response = requests.get(url,headers=headers,) #,proxies = proxies需要时可设置代理
            if response.status_code ==200:
                content = response.text
                tree = etree.HTML(content)
                div_list = tree.xpath('/html/body/section/div[2]/div/div[2]/div')
                for div in div_list:
                    ipv4 = div.xpath('./div[1]/text()')[0]
                    port = div.xpath('./div[2]/text()')[0]
                    ip = ipv4 + ':' + port
                    ip_list.append(copy.deepcopy(ip))
            print(ip_list)
            #不睡封ip
            time.sleep(5)
        ip_set = set(ip_list)  # 去掉可能重复的ip
        ip_list = list(ip_set)
        print(ip_list)
        return ip_list
    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
                print(response.status_code)
                print('测试失败:', proxies)
            except Exception as e:
                print('测试失败:',proxies,e)
                continue
        print('本次向数据库更新数目:',n)
    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self,host,password):
        self.conn = Redis(host=host, port=6379, password=password)
        #爬取代理ip5页
        ip_info = self.crawl_taiyang_ip(5)
        #测试ip是否可用并存储至redis
        self.ip_test(ip_info)

#yqie代理
class YqieProxy():
    def __init__(self):
        # 爬取代理的URL地址，选择的是yqie代理
        self.url_ip = 'http://ip.yqie.com/ipproxy.htm'

    def crawl_yqieIP_ip(self,num):
        '''
        爬取代理ip地址
        '''
        ip_list =[]
        url = self.url_ip
        response = requests.get(url,headers=headers,) #,proxies = proxies需要时可设置代理
        response.encoding= 'utf-8'
        tree = etree.HTML(response.text)
        tr_list = tree.xpath('/html/body/div[1]/div[8]/div[4]/table//tr')
        flag = 1
        for tr in tr_list:
            if flag == 1:
                flag = 0
                pass
            else:
                ipv4 = tr.xpath('./td[1]/text()')[0]
                port = tr.xpath('./td[2]/text()')[0]
                ip = ipv4 + ':' + port
                ip_list.append(copy.deepcopy(ip))
                print(ip)
        ip_set = set(ip_list)  # 去掉可能重复的ip
        ip_list = list(ip_set)
        print(ip_list)
        return ip_list
    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                print(response.status_code)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
                else:
                    print('测试失败:', proxies)
            except Exception as e:
                print('测试失败:',proxies,e)
                pass
        print('本次向数据库更新数目:',n)
    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self,host,password):
        self.conn = Redis(host=host, port=6379, password=password)
        #爬取代理ip1页
        ip_info = self.crawl_yqieIP_ip(1)
        #测试ip是否可用并存储至redis
        self.ip_test(ip_info)

#站大爷代理
class ZdayeProxy():
    def __init__(self):

        self.main_url = 'https://www.zdaye.com/dayProxy/1.html'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
        }
        self.ip_list = []
        url_for_test ='http://httpbin.org/ip'
        #第二页ipURL
        self.ip2_url_list = []
        self.url_list = []
        self.conn = Redis(host='localhost',port=6379)
        self.proxies ={
                'http': '47.108.234.147:8088',  #'http://'+
                }
    def get_detail_url(self,url):
        x = 0
        detail_url_list = []
        main_data = requests.get(url,headers = headers)#,proxies = proxies
        main_data.encoding = 'gbk'
        tree = etree.HTML(main_data.text)
        detail_url_xpath = tree.xpath('//div[@class = "thread_item"]//h3/a/@href')
        for xpath in detail_url_xpath:
            x += 1
            detail_url_list.append(copy.deepcopy('https://www.zdaye.com' + xpath))
            self.url_list.append(copy.deepcopy('https://www.zdaye.com' + xpath))
            if x == 4 :
                break
        return detail_url_list
    def parse_detail_url(self,url):
        detail_url_2 = url.split('.html')[0]+'/2'+'.html'
        self.url_list.append(copy.deepcopy(detail_url_2))
    def parse_ip(self,url):
        data = requests.get(url ,headers = headers)#,proxies = proxies
        data.encoding = 'gbk'
        print(data.text)
        tree = etree.HTML(data.text)
        tr_list = tree.xpath('/html/body/div[3]/div/div[2]/div/div[5]/table//tr')
        for tr in tr_list:
            ipv4 = tr.xpath('./td[1]/text()')[0]
            port = tr.xpath('./td[2]/text()')[0]
            ip = ipv4 + ':' + port
            self.ip_list.append(copy.deepcopy(ip))
            print(ip)
    def ip_test(self,ip_info):
        '''
        测试爬取到的ip，测试成功则存入Redis
        '''
        n = 0
        for ip_for_test in ip_info:
            # 设置代理
            proxies ={
            'http': ip_for_test,  #'http://'+
            }
            try:
                response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=5)
                if response.status_code ==200:
                    print('测试通过:',proxies)
                    x = self.write_to_Redis(ip_for_test)
                    if x == 1:
                        n += 1
            except Exception as e:
                print('测试失败:',proxies)
                continue
        print('本次向数据库更新数目:',n)
    def write_to_Redis(self,proxies):
        '''
        将测试通过的ip存入Redis
        '''
        ex = self.conn.sadd('Proxies',proxies)
        if ex == 1:
            print('Proxies更新成功')
            return 1
        else:
            print('已存在，未更新。')
            return 0

    def proxy_to_redis(self):
        self.get_random_ip()
        for url in self.get_detail_url(self.main_url):
            self.parse_detail_url(url)
        # for url in self.url_list:
        #     self.parse_ip(url)
        #     time.sleep(5)
        self.parse_ip(self.url_list[0])
        print(self.ip_list)
        self.ip_test(url_for_test,self.ip_list)