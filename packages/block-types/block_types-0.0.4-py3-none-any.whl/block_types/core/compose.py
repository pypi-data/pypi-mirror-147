# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/core/compose.ipynb (unless otherwise specified).

__all__ = ['MultiComponent', 'Pipeline', 'Sequential', 'make_pipeline', 'pipeline_factory', 'PandasPipeline',
           'Parallel', 'MultiModality', 'ColumnSelector', 'Concat', 'ColumnTransformer', 'Identity',
           'make_column_transformer_pipelines', 'make_column_transformer', 'MultiSplitComponent', 'MultiSplitDict',
           'MultiSplitDFColumn']

# Cell
import abc
import warnings
import sys
from pathlib import Path
from sklearn.utils import Bunch
import pandas as pd

try:
    from graphviz import *
    imported_graphviz = True
except:
    imported_graphviz = False

from .block_types import (Component,
                                          PandasComponent,
                                          SamplingComponent,
                                          NoSaverComponent)
from .data_conversion import PandasConverter
from .utils import PandasIO
from ..utils.utils import get_logging_level, set_empty_logger
import block_types.config.bt_defaults as dflt

# Cell
class MultiComponent (SamplingComponent):
    """
    Component containing a list of components inside.

    The list must contain at least one component.

    See `Pipeline` class.
    """
    def __init__ (self,
                  *components,
                  separate_labels=dflt.separate_labels,
                  warning_if_nick_name_exists=dflt.warning_if_nick_name_exists,
                  propagate=dflt.propagate,
                  path_results=dflt.path_results,
                  path_models=dflt.path_models,
                  root=None,
                  automatic_root=False,
                  **kwargs):
        """Assigns attributes and calls parent constructor.

        Parameters
        ----------
        separate_labels: bool, optional
            whether or not the fit method receives the labels in a separate `y` vector
            or in the same input `X`, as an additional variable. See description of
            Pipeline class for more details.
        """
        if 'estimator' in kwargs:
            self.logger.warning ('estimator passed as key-word argument in MultiComponent')

        self.warning_if_nick_name_exists = warning_if_nick_name_exists

        if len(components) > 0:
            self.set_components (*components, **kwargs)
        elif not hasattr (self, 'components'):
            self.components = []
        if not hasattr (self, 'finalized_component_list'):
            self.finalized_component_list = False

        if root == True: root = self
        elif automatic_root: root = self if root is None else root
        # we need to call super().__init__() *after* having creating the `components` field,
        # since the constructor of Component calls a method that is overriden in Pipeline,
        # and this method makes use of the mentioned `components` field
        super().__init__ (separate_labels=separate_labels, path_results=path_results, path_models=path_models,
                          root=root, **kwargs)

        self.set_split ('whole')

        self.chain_folders (self.data_io.folder)
        if self.propagate:
            self.set_path_results (self.path_results)
            self.set_path_models (self.path_models)

        self.start_idx = dict (apply = dict (training=0, validation=0, test=0, whole=0),
                               fit = dict (training=0, validation=0, test=0, whole=0))
        self.is_data_source = dict (apply = dict (training=False, validation=False, test=False, whole=False),
                               fit = dict (training=False, validation=False, test=False, whole=False))
        self.all_components_fitted = False
        self.load_all_estimators = False

        if root is not None: self.set_root (root)
        if root is self:
            self.num_names = {}
            self.names = {}
            self.set_unique_names ()

    def __repr__ (self):
        return f'MultiComponent {self.class_name} (name={self.name})'

    def register_components (self, *components):
        """
        Registering component in `self.components` list.

        Every time that a new component is set as an attribute of the pipeline,
        this component is added to the list `self.components`. Same
        mechanism as the one used by pytorch's `nn.Module`
        """
        if not hasattr(self, 'components'):
            self.components = []
            self.finalized_component_list = False
        if not self.finalized_component_list:
            self.components += components

    def _add_named_attribute (self, component, nick_name):
        if not hasattr(self, 'finalized_component_list'):
            self.finalized_component_list = False
        if not hasattr(self, component.name):
            super().__setattr__(component.name, component)
        if not self.finalized_component_list:
            if hasattr(component, 'nick_name') and self.warning_if_nick_name_exists:
                self.logger.warning (f'{component} already has a nick_name: {component.nick_name}')
                warnings.warn (f'{component} already has a nick_name: {component.nick_name}')
            component.nick_name = nick_name

    def __setattr__(self, k, v):
        """
        See register_components
        """
        super().__setattr__(k, v)

        if isinstance(v, Component):
            self.register_components(v)
            self._add_named_attribute (v, k)

    def add_component (self, component):
        if not hasattr(self, 'finalized_component_list'):
            self.finalized_component_list = False
        finalized_component_list = self.finalized_component_list
        self.finalized_component_list = False
        self.register_components(component)
        self._add_named_attribute (component, component.name)
        self.finalized_component_list = finalized_component_list

    def set_components (self, *components, **kwargs):
        self.components = components
        for i, component in enumerate(components):
            component, changed = self.obtain_component (component, **kwargs)
            self._add_named_attribute (component, component.name)
            if changed:
                self.components = list(self.components)
                self.components[i] = component
        self.finalized_component_list = True

    def obtain_component (self, component, **kwargs):
        if isinstance (component, Component):
            return component, False
        elif component.__class__.__name__ == 'function':
            return Component (apply=component, **kwargs), True
        elif isinstance(component, tuple):
            assert (len(component)==2 and isinstance(component[0], str)
                    and component[1].__class__.__name__ == 'function')
            return Component (apply=component[1], class_name=component[0], **kwargs), True
        else:
            return Component (estimator=component, **kwargs), True

    def clear_descendants (self):
        self.cls = Bunch ()
        self.obj = Bunch ()
        self.full_obj = Bunch ()
        self.full_cls = Bunch ()
        for component in self.components:
            if isinstance(component, MultiComponent):
                component.clear_descendants ()

    def gather_descendants (self, root='', nick_name=True):
        if not hasattr (self, 'cls'):
            self.cls = Bunch ()
            self.obj = Bunch ()
            self.full_obj = Bunch ()
            self.full_cls = Bunch ()

        if hasattr(self, 'nick_name'):
            name = self.nick_name if nick_name else self.name
        else:
            name = self.name
        self.hierarchy_path = f'{root}{name}'
        for component in self.components:
            self._insert_descendant (self.cls, component, component.class_name)
            self._insert_descendant (self.obj, component, component.name)

            name = component.nick_name if nick_name else component.name
            component_hierarchy_path = f'{self.hierarchy_path}.{name}'
            self._insert_descendant (self.full_cls, component_hierarchy_path, component.class_name)
            self._insert_descendant (self.full_obj, component_hierarchy_path, component.name)
            if isinstance(component, MultiComponent):
                component.gather_descendants (root=f'{self.hierarchy_path}.',
                                              nick_name=nick_name)
                for name in component.cls:
                    self._insert_descendant (self.cls, component.cls[name], name)
                    self._insert_descendant (self.full_cls, component.full_cls[name], name)
                for name in component.obj:
                    self._insert_descendant (self.obj, component.obj[name], name)
                    self._insert_descendant (self.full_obj, component.full_obj[name], name)

    def _insert_descendant (self, cmp_dict, component, name):
        if name in cmp_dict:
            if not isinstance(cmp_dict[name], list):
                cmp_dict[name] = [cmp_dict[name]]
            if isinstance(component, list):
                cmp_dict[name].extend(component)
            else:
                cmp_dict[name].append(component)
        else:
            if isinstance(component, list):
                cmp_dict[name] = component.copy()
            else:
                cmp_dict[name] = component

    def gather_times (self):
        dfs = [self.profiler.retrieve_times ()]
        for component in self.components:
            if isinstance(component, MultiComponent):
                dfs.append(component.gather_times ())
            else:
                dfs.append(component.profiler.retrieve_times (is_leaf=True))
        dfs = self.profiler.combine_times (dfs)
        return dfs

    def construct_diagram (self, split=None, include_url=False, port=4000, project='block_types'):
        """
        Construct diagram of the pipeline components, data flow and dimensionality.

        By default, we use test data to show the number of observations
        in the output of each component. This can be changed passing
        `split='train'`
        """
        split = self.get_split (split)

        if include_url:
            base_url = f'http://localhost:{port}/{project}'
        else:
            URL = ''

        node_name = 'data'
        output = 'train / test'

        f = Digraph('G', filename='fsm2.svg')
        f.attr('node', shape='circle')

        f.node(node_name)

        f.attr('node', shape='box')
        for component in self.components:
            last_node_name = node_name
            last_output = output
            node_name = component.model_plotter.get_node_name()
            if include_url:
                URL = f'{base_url}/{component.model_plotter.get_module_path()}.html#{node_name}'
            f.node(node_name, URL=URL)
            f.edge(last_node_name, node_name, label=last_output)
            output = component.model_plotter.get_edge_name(split=split)

        last_node_name = node_name
        node_name = 'output'
        f.attr('node', shape='circle')
        f.edge(last_node_name, node_name, label=output)

        return f

    def show_result_statistics (self, split=None):
        """
        Show statistics about results obtained by each component.

        By default, this is shown on test data, although this can change setting
        `split='train'`
        """
        split = self.get_split (split)

        for component in self.components:
            component.show_result_statistics(split=split)

    def show_summary (self, split=None, file=sys.stdout):
        """
        Show list of pipeline components, data flow and dimensionality.

        By default, we use test data to show the number of observations
        in the output of each component. This can be changed passing
        `split='train'`
        """
        split = self.get_split (split)

        node_name = 'data'
        output = 'train / test'
        if isinstance (file, str) or isinstance (file, Path): file = open (file, 'wt')

        for i, component in enumerate(self.components):
            node_name = component.model_plotter.get_node_name()
            output = component.model_plotter.get_edge_name(split=split)
            print (f'{"-"*100}', file=file)
            print (f'{i}: {node_name} => {output}', file=file)


    def get_split (self, split=None):
        if split is None:
            if self.data_io.split is not None:
                split = self.data_io.split
            else:
                split = 'whole'

        return split

    def assert_all_equal (self, path_reference_results, raise_error=False, recursive=True,
                          max_recursion=None, current_recursion=0, verbose=None, **kwargs):
        """Compare results stored in current run against reference results stored in given path."""
        if verbose is not None:
            self.logger.setLevel(get_logging_level (verbose))
        is_equal = True
        non_equal_components = []
        end_recursion = max_recursion is not None and current_recursion >= max_recursion
        components = self.components if not end_recursion else [self]
        for component in components:
            if isinstance(component, MultiComponent) and recursive and not end_recursion:
                this_equal = component.assert_all_equal (path_reference_results,
                                                         raise_error=raise_error,
                                                         recursive=recursive,
                                                         max_recursion=max_recursion,
                                                         current_recursion=current_recursion+1,
                                                         verbose=verbose,
                                                         **kwargs)
            else:
                this_equal = component.assert_equal (path_reference_results,
                                                     raise_error=raise_error,
                                                     verbose=verbose,
                                                     **kwargs)
            if not this_equal:
                non_equal_components.append(component.name)
            is_equal = this_equal and is_equal

        if not is_equal:
            self.logger.warning (f'Results are different in components {non_equal_components}')
        else:
            self.logger.info ('both pipelines give the same results')

        self.logger.setLevel(get_logging_level (self.verbose))

        return is_equal

    def load_estimator (self, skip_from=None):
        for component in self.components[:skip_from]:
            component.load_estimator ()

    def save_result (self, result, split=None, path_results=None, result_file_name=None):
        raise NotImplementedError ()
        self.data_io.save_result (result, split=split, path_results=path_results,
                                  result_file_name=result_file_name)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.save_result (result, split=split, path_results=path_results,
                                       result_file_name=result_file_name)
            else:
                component.data_io.save_result (result, split=split, path_results=path_results,
                                               result_file_name=result_file_name)
    # *************************
    # setters
    # *************************
    def set_split (self, split):
        super().set_split (split)
        for component in self.components:
            component.set_split (split)

    def set_save_splits (self, save_splits):
        super().set_save_splits (save_splits)
        for component in self.components:
            component.set_save_splits (save_splits)

    def set_load_model (self, load_model):
        super().set_load_model (load_model)
        for component in self.components:
            component.set_load_model (load_model)

    def set_save_model (self, save_model):
        super().set_save_model (save_model)
        for component in self.components:
            component.set_save_model (save_model)

    def set_save_result (self, save_result):
        super().set_save_result (save_result)
        for component in self.components:
            component.set_save_result (save_result)

    def set_load_result (self, load_result):
        super().set_load_result (load_result)
        for component in self.components:
            component.set_load_result (load_result)

    def set_path_results (self, path_results):
        self.data_io.set_path_results (path_results)
        for component in self.components:
            if not component.data_io.stop_propagation:
                if isinstance (component, MultiComponent):
                    component.set_path_results (path_results)
                else:
                    component.data_io.set_path_results (path_results)
    def set_path_models (self, path_models):
        self.data_io.set_path_models (path_models)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_path_models (path_models)
            else:
                component.data_io.set_path_models (path_models)
    def chain_folders (self, folder, root=True):
        if folder == '':
            return
        if root:
            self.data_io.chain_folders ('')
        else:
            self.data_io.chain_folders (folder)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.chain_folders (folder, root=False)
            else:
                component.data_io.chain_folders (folder)

    def set_root (self, root):
        self.root = root
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_root (root)
            else:
                component.root = root

    def register_global_name (self, component):
        if component.name in self.root.names:
            if component.name not in self.root.num_names:
                self.root.num_names[component.name] = 0
            self.root.num_names[component.name] += 1
            component.set_name (f'{component.name}_{self.root.num_names[component.name]}')
        self.root.names[component.name] = component

    def set_unique_names (self):
        self.register_global_name (self)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_unique_names ()
            else:
                self.register_global_name (component)

    def find_last_result (self, split=None):
        return False

    def find_last_fitted_model (self, split=None):
        return False

# Cell
class Pipeline (MultiComponent):
    """
    Pipeline composed of a list of components that run sequentially.

    During training, the components of the list are trained one after the other,
    where one component is fed the result of transforming the data with the list
    of components located before in the pipeline.

    The `Pipeline` class is a subclass of `SamplingComponent`, which itself is a
    subclass of `Component`. This provides the functionality of `Component`
    to any implemented pipeline, such as logging the messages, loading / saving the
    results, and convert the data format so that it can work as part of other
    pipelines with potentially other data formats.

    Being a subclass of `SamplingComponent`, the `transform` method
    receives an input data  `X` that contains both data and labels.

    Furthermore, the Pipeline constructor sets `separate_labels=False` by default,
    which means that the `fit` method also receives an input data `X` that contains
    not only data but also labels. This is necessary because some of the components in
    the pipeline might be of class `SamplingComponent`, and such components
    need the input data `X` to contain labels when calling `transform` (and note that
    this method is called when calling `fit` on a pipeline, since we do `fit_transform`
    on all the components except for the last one)
    """
    def __init__ (self, *components, **kwargs):
        """Assigns attributes and calls parent constructor.

        Parameters
        ----------
        separate_labels: bool, optional
            whether or not the fit method receives the labels in a separate `y` vector
            or in the same input `X`, as an additional variable. See description of
            Pipeline class for more details.
        """

        super().__init__ (*components, **kwargs)

    def __repr__ (self):
        return f'Sequential {self.class_name} (name={self.name})'

    def _fit (self, X, y=None):
        """
        Fit components of the pipeline, given data X and labels y.

        By default, y will be None, and the labels are part of `X`, as a variable.
        """
        X = self._fit_apply (X, y=y, last=-1)
        self.components[-1].fit (X, y=y)

    def _fit_apply (self, X, y=None, split=None, last=None, **kwargs):
        split = self.data_io.split if split is None else split
        first = self.start_idx['fit'][split]
        if first >= len(self.components):
            return X
        if first > 0:
            self.load_estimator (skip_from=first)
        for component in self.components[first:last]:
            X = component.fit_apply (X, y=y, **kwargs)
        return X

    def _apply (self, *X, split=None):
        """Transform data with components of pipeline, and predict labels with last component.

        In the current implementation, we consider prediction a form of mapping,
        and therefore a special type of transformation."""
        split = self.data_io.split if split is None else split
        first = self.start_idx['apply'][split]

        if first < len(self.components):
            component = self.components[first]
            X = component (*X)
            first += 1
        if first >= len(self.components):
            return X
        for component in self.components[first:]:
            X = component (X)

        return X

    def find_last_result (self, split=None, func='apply', first=-1):

        idx = None
        for i, component in enumerate(self.components[first::-1]):
            if component.data_io.can_load_result () and component.data_io.exists_result (split=split):
                idx = i
                break
            elif isinstance (component, MultiComponent):
                starting_point = component.find_last_result (split=split)
                if starting_point:
                    idx = i
                    break
        split = self.data_io.split if split is None else split
        if idx is not None:
            first = (len(self.components) + first) if (first < 0) else first
            self.start_idx[func][split] = first - idx
            self.is_data_source[func][split] = True
        else:
            self.start_idx[func][split] = 0
            self.is_data_source[func][split] = False
        return self.is_data_source[func][split]

    def find_last_fitted_model (self, split=None):
        idx = len(self.components)-1
        all_components_fitted = True
        self.load_all_estimators = False
        for i, component in enumerate(self.components):
            if isinstance (component, MultiComponent):
                if not component.find_last_fitted_model (split=split):
                    idx = i-1
                    all_components_fitted = False
                    break
            elif (component.is_model and
                  not (component.data_io.can_load_model () and component.data_io.exists_estimator ())):
                    idx = i-1
                    all_components_fitted = False
                    break

        if idx >= 0:
            _ = self.find_last_result (split=split, func='fit', first=idx)
        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted
        return all_components_fitted

# Sequential is an alias of Pipeline
Sequential = Pipeline

# Cell
def make_pipeline(*components, cls=Pipeline, **kwargs):
    """Create `Pipeline` object of class `cls`, given `components` list."""
    pipeline = cls (**kwargs)
    pipeline.set_components(*components)
    return pipeline

# Cell
def pipeline_factory (pipeline_class, **kwargs):
    """Creates a pipeline object given its class `pipeline_class`

    Parameters
    ----------
    pipeline_class : class or str
        Name of the pipeline class used for creating the object.
        This can be either of type string or class.
    """
    if type(pipeline_class) is str:
        Pipeline = eval(pipeline_class)
    elif type(pipeline_class) is type:
        Pipeline = pipeline_class
    else:
        raise ValueError (f'pipeline_class needs to be either string or class, we got {pipeline_class}')

    return Pipeline (**kwargs)

# Cell
class PandasPipeline (Pipeline):
    """
    Pipeline that saves results in parquet format, and preserves DataFrame format.

    See `Pipeline` class for an explanation of using `separate_labels=False`
    """
    def __init__ (self,
                  data_converter='PandasConverter',
                  data_io='PandasIO',
                  separate_labels=False,
                  **kwargs):
        super().__init__ (data_converter=data_converter,
                          data_io=data_io,
                          separate_labels=separate_labels,
                          **kwargs)

# Cell
class Parallel (MultiComponent):
    """
    List of components that don't have sequential dependencies.

    As the name suggests, these components could run in parallel,
    if a concurrency mechanism is employed.
    """
    def __init__ (self, *components, select_input_to_fit=None, select_input=None,
                  initialize_result=None, join_result=None, finalize_result=None, **kwargs):
        """Assigns attributes and calls parent constructor.
        """

        select_input_to_fit = (self.select_input_to_fit if select_input_to_fit is None
                                      else select_input_to_fit)
        self.select_input_to_fit = select_input_to_fit
        select_input = (self.select_input if select_input is None else select_input)
        self.select_input = select_input
        initialize_result = (self.initialize_result if initialize_result is None
                                      else initialize_result)
        self.initialize_result = initialize_result
        join_result = (self.join_result if join_result is None
                                      else join_result)
        self.join_result = join_result
        finalize_result = (self.finalize_result if finalize_result is None
                                      else finalize_result)
        self.finalize_result = finalize_result

        super().__init__ (*components, **kwargs)

    def __repr__ (self):
        return f'Parallel {self.class_name} (name={self.name})'

    def select_input_to_fit (self, X, y, components, i):
        return X, y

    def _fit (self, X, y=None):
        """
        Fit components of the pipeline, given data X and labels y.

        By default, y will be None, and the labels are part of `X`, as a variable.
        """
        for i, component in enumerate(self.components):
            Xi, yi = self.select_input_to_fit (X, y, self.components, i)
            component.fit (Xi, y=yi, **kwargs)

    def initialize_result (self):
        return []

    def select_input (self, X, components, i):
        return X

    def join_result (self, Xr, Xi_r, components, i):
        Xr.append (Xi_r)
        return Xr

    def finalize_result (self, Xr, components=None):
        return Xr

    def _apply (self, *X):
        """Transform data with components of pipeline, and predict labels with last component.

        In the current implementation, we consider prediction a form of mapping,
        and therefore a special type of transformation."""
        Xr = self.initialize_result ()
        for i, component in enumerate(self.components):
            Xi = self.select_input (X, self.components, i)
            Xi_r = component (*Xi)
            Xr = self.join_result (Xr, Xi_r, self.components, i)

        Xr = self.finalize_result (Xr)

        return Xr

    def _fit_apply (self, X, y=None, **kwargs):
        Xr = self.initialize_result ()
        for i, component in enumerate(self.components):
            Xi, yi = self.select_input_to_fit (X, y, self.components, i)
            Xi_r = component.fit_apply (Xi, y=yi, **kwargs)
            Xr = self.join_result (Xr, Xi_r, self.components, i)

        Xr = self.finalize_result (Xr)

        return Xr

    def find_last_result (self, split=None):
        self.is_data_source = True
        for i, component in enumerate(self.components):
            if not (component.data_io.can_load_result () and component.data_io.exists_result (split=split)):
                if isinstance (component, MultiComponent):
                    self.is_data_source = self.is_data_source and component.find_last_result (split=split)
                else:
                    self.is_data_source = False
        return self.is_data_source

    def find_last_fitted_model (self, split=None):
        self.load_all_estimators = False
        all_components_fitted = True
        for i, component in enumerate(self.components):
            if isinstance (component, MultiComponent):
                if not component.find_last_fitted_model (split=split):
                    all_components_fitted = False
            elif (component.is_model and
                  not (component.data_io.can_load_model () and component.data_io.exists_estimator ())):
                    all_components_fitted = False
        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted
        return all_components_fitted

# Cell
class MultiModality (Parallel):
    """
    Analyzes multiple modalities using Parallel data flow.
    """
    def __init__ (self, *components, use_name=False, component_class=None, configs=None, **kwargs):
        """Assigns attributes and calls parent constructor.
        """

        if component_class is not None and configs is not None and isinstance(configs, dict):
            new_components = []
            for k in configs:
                new_components.append (component_class (**configs[k], folder=k, suffix=k))
            components = list(components) + new_components
        super().__init__ (*components, **kwargs)
        for component in components:
            component.key = component.name if use_name else component.data_io.folder

    def __repr__ (self):
        return f'MultiModality {self.class_name} (name={self.name})'

    def select_input_to_fit (self, X, y, components, i):
        return X[components[i].key], y

    def initialize_result (self):
        return {component.key: None for component in self.components}

    def select_input (self, X, components, i):
        return X[components[i].key]

    def join_result (self, Xr, Xi_r, components, i):
        Xr[components[i].key] = Xi_r
        return Xr

# Cell
class ColumnSelector (NoSaverComponent):
    def __init__ (self,
                  columns=[],
                  remainder=False,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, df, **kwargs):
        if self.remainder:
            return df[[c for c in df.columns if c not in self.columns]]
        else:
            return df[self.columns]

# Cell
class Concat (NoSaverComponent):
    def __init__ (self,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, *dfs, **kwargs):
        return pd.concat(list(dfs), axis=1)

# Cell
class _BaseColumnTransformer (MultiComponent):
    def __init__ (self, name=None, class_name=None, direct_apply=True, direct_fit=True,**kwargs):
        super().__init__ (name=name, class_name=class_name, direct_apply=direct_apply,
                          direct_fit=direct_fit, **kwargs)
        self.concat = Concat (**kwargs)
        del self.concat.nick_name

    def set_components (self, *components):
        components = list(components)
        components.append (self.concat)
        super().set_components (*components)

    def _fit (self, df, y=None, **kwargs):
        assert len(self.components) > 0
        assert self.components[-1] is self.concat
        for component in self.components[:-1]:
            component.fit (df, **kwargs)
        return self

    def _apply (self, df, **kwargs):
        dfs = []
        assert len(self.components) > 0
        assert self.components[-1] is self.concat
        for component in self.components[:-1]:
            dfs.append (component.transform (df, **kwargs))
        df_result = self.concat.transform (*dfs)
        return df_result

class ColumnTransformer (_BaseColumnTransformer):
    def __init__ (self, *transformers, remainder = 'drop', **kwargs):
        super().__init__ (**kwargs)
        components = make_column_transformer_pipelines (*transformers, remainder=remainder, **kwargs)
        super().set_components(*components)

# Cell
class Identity (NoSaverComponent):
    def __init__ (self,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, X, **kwargs):
        return X

# Cell
def _append_pipeline (pipelines, name, transformer, columns, remainder= False, **kwargs):
    drop = False
    if isinstance(transformer, str):
        if transformer == 'passthrough':
            transformer = Identity (**kwargs)
        elif transformer == 'drop':
            drop = True
        else:
            raise ValueError (f'name {transformer} not recognized')

    if not drop:
        config=kwargs.copy()
        config.update({name:dict(data_io='NoSaverIO')})
        config.update (direct_apply=True)
        pipeline = make_pipeline(ColumnSelector(columns, remainder=remainder, **kwargs),
                                 transformer,
                                 name=name,
                                 **config)
        pipelines.append (pipeline)

def _get_transformer_name (transformer, columns):
    columns_name = ''.join([x[0] for x in columns])
    if len(columns_name) > 5:
        columns_name = columns_name[:5]
    if isinstance(transformer,str):
        if transformer == 'passthrough':
            transformer_name = 'pass'
        elif transformer == 'drop':
            transformer_name = 'drop'
        else:
            raise ValueError (f'name {transformer} not recognized')
    elif hasattr(transformer, 'name'):
        transformer_name = transformer.name
    else:
        transformer_name = transformer.__class__.__name__
    name = f'{transformer_name}_{columns_name}'
    return name

def make_column_transformer_pipelines (*transformers, remainder='drop', **kwargs):
    pipelines = []
    all_columns = []
    for name, transformer, columns in transformers:
        _append_pipeline (pipelines, name, transformer, columns, **kwargs)
        all_columns.extend(columns)

    all_columns = list(set(all_columns))
    name = _get_transformer_name (remainder, ['r','e','m'])
    _append_pipeline (pipelines, name, remainder, all_columns, remainder=True, **kwargs)

    return pipelines

def make_column_transformer (*transformers, remainder='drop', name=None, class_name=None, **kwargs):
    transformers_with_name = []
    for transformer, columns in transformers:
        transformer_name = _get_transformer_name (transformer, columns)
        transformers_with_name.append ((transformer_name, transformer, columns))

    pipelines = make_column_transformer_pipelines (*transformers_with_name,
                                                   remainder=remainder,
                                                   **kwargs)
    column_transformer = _BaseColumnTransformer (name=name, class_name=class_name, **kwargs)
    column_transformer.set_components(*pipelines)
    return column_transformer


# Cell
class MultiSplitComponent (MultiComponent, metaclass=abc.ABCMeta):
    def __init__ (self,
                  component=None,
                  name=None,
                  class_name=None,
                  fit_to = 'training',
                  fit_additional = [],
                  apply_to = ['training', 'validation', 'test'],
                  raise_error_if_split_doesnot_exist=False,
                  raise_warning_if_split_doesnot_exist=True,
                  **kwargs):
        if class_name is None:
            if hasattr(component, 'class_name'):
                class_name = f'{component.class_name}MultiSplit'
            else:
                class_name = f'{component.__class__.__name__}MultiSplit'

        if name is None:
            if hasattr(component, 'name'):
                name = f'{component.name}_multi_split'
            else:
                name = f'{component.__class__.__name__}_multi_split'

        super().__init__ (name=name, class_name=class_name, **kwargs)

    @abc.abstractmethod
    def _initialize_fit (self, X):
        pass

    @abc.abstractmethod
    def _include_split_in_fit (self, additional_data, split, X):
        pass

    @abc.abstractmethod
    def _select_training_split (self, X):
        pass

    def _fit (self, X, y=None):
        X = self._initialize_fit (X)
        component = self.components[0]
        additional_data = {}
        for split in self.fit_additional:
            if split not in ['validation', 'test']:
                raise ValueError (f'split {split} not valid')
            self._include_split_in_fit (additional_data, split, X)
        X_fit = self._select_training_split (X)
        component.fit(X_fit, y=y, split=self.fit_to, **additional_data)

    @abc.abstractmethod
    def _get_split_keys (self, X):
        pass

    def _issue_error_or_warning (self, split, X):
        message = f'split {split} not found in X ({self._get_split_keys (X)})'
        if self.raise_error_if_split_doesnot_exist:
            raise RuntimeError (message)
        elif self.raise_warning_if_split_doesnot_exist:
            warnings.warn (message)

    @abc.abstractmethod
    def _initialize_apply (self, X, apply_to, split):
        pass

    @abc.abstractmethod
    def _included_split (self, split, X):
        pass

    @abc.abstractmethod
    def _select_split (self, X, split):
        pass

    @abc.abstractmethod
    def _convert_result (self, result, input_not_dict, output_not_dict):
        pass

    @abc.abstractmethod
    def _initialize_result (self):
        pass

    @abc.abstractmethod
    def _add_result (self, result, split, result_split):
        pass

    def _apply (self, X, apply_to = None, output_not_dict=False, split=None, **kwargs):
        apply_to = self.apply_to if apply_to is None else apply_to
        apply_to = apply_to if isinstance(apply_to, list) else [apply_to]
        X, input_not_dict = self._initialize_apply (X, apply_to, split)

        component = self.components[0]
        result = self._initialize_result ()

        for split in apply_to:
            if self._included_split (split, X):
                X_split = self._select_split (X, split)
                result_split = component.apply (X_split, split=split, **kwargs)
                result = self._add_result (result, split, result_split)
            else:
                self._issue_error_or_warning (split, X)

        result = self._convert_result (result, input_not_dict, output_not_dict)
        return result

    def find_last_result (self, apply_to = None, split=None, **kwargs):
        apply_to = self.apply_to if apply_to is None else apply_to
        apply_to = apply_to if isinstance(apply_to, list) else [apply_to]

        self.is_data_source = True
        for split in apply_to:
            if not (component.data_io.can_load_result () and component.data_io.exists_result (split=split)):
                if isinstance (component, MultiComponent):
                    # TODO: have one flag is_data_source per split
                    # or make MultiSplitComponent a Parallel object
                    # or always use DataFrame
                    self.is_data_source = self.is_data_source and component.find_last_result (split=split)
                else:
                    self.is_data_source = False
        return self.is_data_source

    def find_last_fitted_model (self, apply_to = None, split=None, **kwargs):
        all_components_fitted = True
        self.load_all_estimators = False
        split = self.fit_to
        if isinstance (component, MultiComponent):
            if not component.find_last_fitted_model (split=split):
                all_components_fitted = False
        elif (component.is_model and
              not (component.data_io.can_load_model () and component.data_io.exists_model ())):
                all_components_fitted = False

        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted

        return all_components_fitted

# Cell
class MultiSplitDict (MultiSplitComponent):
    def __init__ (self, component=None, **kwargs):
        super().__init__ (component=component, **kwargs)

    def __repr__ (self):
        return f'MultiSplitDict {self.class_name} (name={self.name})'

    def _initialize_fit (self, X):
        if not isinstance(X, dict):
            X = {self.fit_to: X}
        return X

    def _include_split_in_fit (self, additional_data, split, X):
        if split in X.keys():
            additional_data[f'{split}_data'] = X[split]
        else:
            self._issue_error_or_warning (split, X)

    def _select_training_split (self, X):
        return X[self.fit_to]

    def _get_split_keys (self, X):
        return X.keys()

    def _initialize_apply (self, X, apply_to, split):
        if not isinstance(X, dict):
            key = apply_to[0] if len(apply_to)==1 else split if split is not None else 'test'
            X = {key: X}
            input_not_dict = True
            self.key = key
        else:
            input_not_dict = False

        return X, input_not_dict

    def _included_split (self, split, X):
        return split in X.keys()

    def _select_split (self, X, split):
        return X[split]

    def _initialize_result (self):
        return {}

    def _add_result (self, result, split, result_split):
        result[split] = result_split
        return result

    def _convert_result (self, result, input_not_dict, output_not_dict):
        if input_not_dict:
            result = result[self.key]
        elif output_not_dict and len(result)==1:
            result = list(result.items())[0][1]
        return result

# Cell
class MultiSplitDFColumn (MultiSplitComponent):
    def __init__ (self, component=None, **kwargs):
        super().__init__ (component=component, **kwargs)

    def __repr__ (self):
        return f'MultiSplitDF {self.class_name} (name={self.name})'

    def _initialize_fit (self, X):
        if 'split' not in X.columns:
            # X.index = pd.MultiIndex.from_arrays ([[self.fit_to]*X.shape[0], X.index], names=('split', 'number'))
            X['split'] = self.fit_to
        return X

    def _include_split_in_fit (self, additional_data, split, X):
        if split in X.split.values:
            additional_data[f'{split}_data'] = X[X.split==split]
        else:
            self._issue_error_or_warning (split, X)

    def _select_training_split (self, X):
        return X[X.split==self.fit_to]

    def _get_split_keys (self, X):
        #X.index.get_level_values(0).unique()
        return X.split.unique()

    def _initialize_apply (self, X, apply_to, split):
        if 'split' not in X.columns:
            key = apply_to[0] if len(apply_to)==1 else split if split is not None else 'test'
            X['split'] = key
            input_not_dict = True
            self.key = key
        else:
            input_not_dict = False

        return X, input_not_dict

    def _included_split (self, split, X):
        #split in X.index.get_level_values(0).values
        return split == 'whole' or split in X.split.values

    def _select_split (self, X, split):
        return X[X.split==split] if split != 'whole' else X

    def _initialize_result (self):
        return []

    def _add_result (self, result, split, result_split):
        result_split['split']=split
        result.append (result_split)
        return result

    def _convert_result (self, result, input_not_dict, output_not_dict):
        result = pd.concat (result, axis=0)
        return result