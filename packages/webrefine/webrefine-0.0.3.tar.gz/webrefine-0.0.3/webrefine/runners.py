# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_runners.ipynb (unless otherwise specified).


from __future__ import annotations


__all__ = ['Process', 'RunnerMemory', 'minibatch', 'compress_encode', 'compress_decode', 'RunnerCached']

# Cell
#nbdev_comment from __future__ import annotations
from dataclasses import dataclass
from typing import Callable



# Cell
import logging
from tqdm.auto import tqdm

@dataclass
class Process:
    queries: list[Callable]
    steps: list[Callable]
    filter: Callable

# Cell

class RunnerMemory():
    def __init__(self, process: Process, progress_bar: bool = True):
        self.process = process
        self.progress_bar = progress_bar

    def query(self):
        for query in tqdm(self.process.queries, desc='query', disable=not self.progress_bar):
            for record in query.query():
                yield record

    def prepare(self, records):
        return self.process.filter(tqdm(records, desc='filter', disable=not self.progress_bar))

    def fetch(self, records):
        for record in tqdm(records, desc='fetch', disable=not self.progress_bar):
            yield (record.content, record)

    def transform(self, content_records):
        for content, record in tqdm(content_records, desc='transform', disable=not self.progress_bar):
            error = False
            for step in self.process.steps:
                try:
                    content = step(content, record)
                except Exception as e:
                    error = True
                    logging.error('Error processing %s at step %s: %s' % (record, step.__name__, e))
                    break
            if not error:
                yield content

    def run(self):
        records = self.prepare(self.query())
        content_records = self.fetch(records)
        return self.transform(content_records)

# Cell
import itertools
from pathlib import Path
from sqlitedict import SqliteDict

def minibatch(seq, size):
    items = []
    for x in seq:
        items.append(x)
        if len(items) >= size:
            yield items
            items = []
    if items:
        yield items

import zlib, pickle, sqlite3
def compress_encode(obj: bytes):
     return sqlite3.Binary(zlib.compress(obj))
def compress_decode(obj):
     return zlib.decompress(bytes(obj))

class RunnerCached():
    def __init__(self, process: Process, path: Union[str, Path], progress_bar: bool = True, batch_size: int = 1024):
        self.process = process
        self.progress_bar = progress_bar
        self.batch_size = batch_size

        self.path = Path(path)

        self._query = SqliteDict(path, tablename='query', autocommit=True)
        self._fetch = SqliteDict(path, tablename='fetch', autocommit=False, encode=compress_encode, decode=compress_decode)

    def query(self):
        # TODO: Don't cache WaybackQuery or FileQuery
        for query in tqdm(self.process.queries, desc='query', disable=not self.progress_bar):
            key = repr(query)
            if key not in self._query:
                self._query[key] = list(query.query())

        # TODO: Merge
        for query in self.process.queries:
            for record in self._query[key]:
                yield record

    def prepare(self, records):
        return self.process.filter(tqdm(records, desc='filter', disable=not self.progress_bar))

    def fetch(self, records):
        records = list(records)
        fetched = set(self._fetch.keys())
        unfetched_records = [r for r in records if r.digest not in fetched]
        unfetched_records = sorted(unfetched_records, key=lambda x: str(type(x)))

        with tqdm(total=len(unfetched_records), desc='fetch') as pbar:
            for cls, record_group in itertools.groupby(unfetched_records, key=type):
                for record_group_batch in minibatch(record_group, self.batch_size):
                    record_group_batch = list(record_group_batch)
                    for content, record in zip(cls.fetch_parallel(record_group_batch,
                                                                  callback=lambda r, c: pbar.update(1)),
                                               record_group_batch):
                        assert record.digest is not None
                        self._fetch[record.digest] = content
                    self._fetch.commit()

        for record in records:
            yield (self._fetch[record.digest], record)


    def transform(self, content_records):
        for content, record in tqdm(content_records, desc='transform', disable=not self.progress_bar):
            error = False
            for step in self.process.steps:
                try:
                    content = step(content, record)
                except Exception as e:
                    error = True
                    logging.error('Error processing %s at step %s: %s' % (record, step.__name__, e))
                    break
            if not error:
                yield content

    def run(self):
        records = self.prepare(self.query())
        content_records = self.fetch(records)
        return self.transform(content_records)