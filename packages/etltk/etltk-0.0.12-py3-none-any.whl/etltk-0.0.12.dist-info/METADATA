Metadata-Version: 2.1
Name: etltk
Version: 0.0.12
Summary: Ethiopian Language NLP Toolkit
Home-page: https://github.com/robikieq/ethiopian_language_toolkit.git
Author: Robel Equbasilassie
Author-email: robiki4life@gmail.com
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE.txt
Provides-Extra: emoji_
Requires-Dist: 1.7.0 ; extra == 'emoji_'
Requires-Dist: Emoji ; extra == 'emoji_'
Provides-Extra: textsearch_
Requires-Dist: 0.0.21 ; extra == 'textsearch_'
Requires-Dist: TextSearch ; extra == 'textsearch_'

# Ethiopian Language Toolkit (etltk)

- The Ethiopian Natural Language Toolkit (ETLTK) project aimed to develop a suite of open source Natural Language Processing modules for the Ethiopian language.

- Under construction! Not ready for use yet! Currently experimenting :-)

## Installation

### pip

- **etltk** supports Python 3.6 or later. We recommend that you install etltk via `pip`, the Python package manager. To install, simply run:

  ```python
    pip install etltk
  ```

### From Source

- Alternatively, you can also install from source via ethiopian_language_toolkitРђЎs git repository, which will give you more flexibility in developing on top of etltk. For this option, run

  ```python
    git clone https://github.com/robikieq/ethiopian_language_toolkit.git
    
    cd ethiopian_language_toolkit
    
    pip install -e .
  ```

## Usage

1. Text Annotating with AmharicDocument

- Annotating amharic text is very simple: you can simply pass the text to the `AmharicDocument` and access all annotations from the returned AmharicDocument object:
  - Within AmharicDocument, annotations are further stored in `Sentences`, `Tokens`, `Words`.
  
  ```python
    from etltk import AmharicDocument

    # Annotating a Document
    sample_text = """
    рѕџрІФрІЮрІФ 14рЇБ 2014 рІЊ.рѕЮ рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх /Artificial Intelligence/ ріарѕЂріЋ ріФрѕѕрЅарЅх рІЮрЅЁрЅ░ріЏ рІ░рѕерїЃ рІѕрІ░ рѕІрЅђ рІ░рѕерїЃ рѕѕрѕЏрІхрѕерѕхрЇБ рѕЃрїѕрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрІЊрѕѕрѕЮ рЅ░рІ░рѕФрѕй рѕѕрѕЏрІхрѕерїЇрЇБ ріарїѕрѕФрІі ріарЅЁрѕЮріЋ рѕѕрѕЏрѕ│рІ░рїЇ ріЦріЊ рЅ░рїарЅЃрѕџ рѕѕрѕўрѕєріЋ рЅарїІрѕФ ріарЅЦрѕ« рѕўрѕхрѕФрЅ▒ ріЦрїЁрїЇ рїарЅЃрѕџ ріљрІЇрЇАрЇА

    рЅарѕЏрѕйріЋ рІЊрѕхрЅ░рѕЮрѕ« (Machine Learning) ріарѕЏріФріЮріљрЅх рІерїйрѕЂрЇЇ ріЊрѕЎріЊрІјрЅй рЅаріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рѕЦрѕГрІЊрЅх рѕѕрѕЏрѕ░рѕЇрїаріЋрЇБ рІерїйрѕЂрЇЇ рІ│рЅ│ріЋ рѕўрѕ░рЅЦрѕ░рЅЦ ріЦріЊ рѕЏрІ░рѕФрїђрЅхрЇц рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅйріЋ /Natural Language Processing Tools/ рЅарѕўрїарЅђрѕЮ рІерїйрѕЂрЇЇ рІ│рЅ│ріЋ рЇЋрѕ«рѕ░рѕх рѕЏрІхрѕерїЇ рЅ░рЅђрІ│рѕџ ріЦріЊ рѕўрѕ░рѕерЅ│рІі рїЅрІ│рІГ ріљрІЇрЇб

    рІерѕЏрѕйріЋ рѕѕрѕГріњріЋрїЇ рѕхрѕЇрЅ░-рЅђрѕўрѕ«рЅй рЅарѕўрїарЅђрѕЮ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅх ріЦріЊ рѕўрѕерІ│рЅхрЇБ рІерїйрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅхрЇБ рІерЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІерѕЃрїѕрѕфріЏ ріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅйрЇБ рѕхрѕЇрЅ░-рЅђрѕўрѕ«рЅй ріЦріЊ рѕърІ┤рѕјрЅйріЋ рѕЏрІўрїІрїђрЅх рЅ░рїѕрЅб ріљрІЇрЇб рЅарІџрѕЁрѕЮ рѕўрѕ░рѕерЅх ріарѕЏрѕГріЏрЇБ ріарЇІріЋ рідрѕ«рѕърЇБ рѕХрѕЏрѕіріЏ ріЦріЊ рЅхрїЇрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрѕЏрѕйріЋ рІерѕЏрѕхрЅ░рѕЏрѕГ рѕѓрІ░рЅхріЋ рЅђрѕІрѕЇріЊ рІерЅ░рЅђрѕІрЅ░рЇЇ ріЦріЋрІ▓рѕєріЋ рІФрѕхрЅйрѕІрѕЇрЇАрЇА

    рІерЅхрѕГрїЅрѕЮ ріарїѕрѕЇрїЇрѕјрЅхрЇБ рЅ╗рЅхрЅдрЅх (рІерІЇрІГрІГрЅх рѕўрѕѕрІІрІѕрїФ рѕ«рЅдрЅх) рЇА рІерЇЁрѕЂрЇЇ рѕ░ріљрІХрЅй рѕЇрІерЅ│рЇБ рІерЅЃрѕІрЅх рЅхріГріГрѕѕріЏріљрЅхріЋ рѕЏрѕерїІрїѕрїЦрЇБ рЅарЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рЅхріЋрЅ░ріЊ рѕўрѕарѕерЅх рїйрѕЂрЇјрЅйріЋ рѕѕрѕЏрІІрЅђрѕГ ріЦріЊ рѕѕрѕўрѕўрѕхрѕерЅхрЇБ рѕерїЁрѕЮ рїйрѕЂрЇјрЅйріЋ рѕЏрѕ│рїарѕГрЇБ ріаріЋрі│рѕГ рїЅрІ│рІ«рЅйріЋ рѕўрѕѕрІерЅх рІѕрІГрѕЮ рїЦрЅЁрѕЇ рѕЃрѕ│рЅЦ рѕЏрІЇрїБрЅхрЇБ ріЋрїЇрїЇрѕГріЋ рІѕрІ░ рїйрѕЂрЇЇ рІерѕџрЅђрІГрѕЕ ріарїѕрѕЇрїЇрѕјрЅХрЅйріЋ рІерѕџрѕ░рїА рѕўрЅ░рїЇрЅарѕфрІФ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїі ріљрІЇрЇб

    рІеріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рЅ┤ріГріќрѕјрїѓ рІўрѕГрЇЇ рЅарїЦріЊрЅхрЇБ рѕЮрѕГрѕЮріЊ рѕЇрѕЏрЅх ріЦріЋрІ▓рѕўрѕФ рѕЏрѕхрЅ╗рѕЇрЇБ ріарѕхрЇѕрѕІрїі рІерІ▓рїѓрЅ│рѕЇ рѕўрѕарѕерЅ░ рѕЇрѕЏрЅХрЅй рѕЏрѕЎрѕЈрЅх рѕўрѕ░рѕерЅ│рІі рѕѕрІЇрїЦ рѕѕрѕЏрѕЮрїБрЅхрЇБ ріЦріЊ рІеріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рІЇрїцрЅХрЅйріЋ рЅарЅ░рїЇрЅБрѕГ рЅарѕхрЇІрЅх рїЦрЅЁрѕЮ рѕІрІГ ріЦріЋрІ▓рІЅрѕЅ рІГрѕерІ│рѕЇрЇб ­Ъцћ
    """

    doc = AmharicDocument(sample_text)

    # The following example shows how to print the `clean` text:
    print(doc)
    
    # output: AmharicDocument("рѕџрІФрІЮрІФ рІЊрѕўрЅ░ рѕЮрѕЁрѕерЅх рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх ріарѕЂріЋ ріФрѕѕрЅарЅх рІЮрЅЁрЅ░ріЏ рІ░рѕерїЃ рІѕрІ░ рѕІрЅђ рІ░рѕерїЃ рѕѕрѕЏрІхрѕерѕх рѕђрїѕрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕріарѕѕрѕЮ рЅ░рІ░рѕФрѕй рѕѕрѕЏрІхрѕерїЇ ріарїѕрѕФрІі ріарЅЁрѕЮріЋ рѕѕрѕЏрѕ│рІ░рїЇ ріЦріЊ рЅ░рїарЅЃрѕџ рѕѕрѕўрѕєріЋ рЅарїІрѕФ ріарЅЦрѕ« рѕўрѕхрѕФрЅ▒ ріЦрїЁрїЇ рїарЅЃрѕџ ріљрІЇ рЅарѕЏрѕйріЋ ріарѕхрЅ░рѕЮрѕ« ріарѕЏріФріЮріљрЅх рІерЇЁрѕЂрЇЇ ріЊрѕЎріЊрІјрЅй рЅаріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рѕхрѕГріарЅх рѕѕрѕЏрѕ░рѕЇрїаріЋ рІерЇЁрѕЂрЇЇ рІ│рЅ│ріЋ рѕўрѕ░рЅЦрѕ░рЅЦ ріЦріЊ рѕЏрІ░рѕФрїђрЅх рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅйріЋ рЅарѕўрїарЅђрѕЮ рІерЇЁрѕЂрЇЇ рІ│рЅ│ріЋ рЇЋрѕ«рѕ░рѕх рѕЏрІхрѕерїЇ рЅ░рЅђрІ│рѕџ ріЦріЊ рѕўрѕ░рѕерЅ│рІі рїЅрІ│рІГ ріљрІЇ рІерѕЏрѕйріЋ рѕѕрѕГріњріЋрїЇ рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй рЅарѕўрїарЅђрѕЮ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅхріЊ рѕўрѕерІ│рЅх рІерЇЁрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅх рІерЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй ріЦріЊ рѕърІ┤рѕјрЅйріЋ рѕЏрІўрїІрїђрЅх ріарѕхрЇѕрѕІрїі ріљрІЇ рІерЅхрѕГрїЅрѕЮ ріарїѕрѕЇрїЇрѕјрЅх рЅ╗рЅхрЅдрЅх рІерІЇрІГрІГрЅх рѕўрѕѕрІІрІѕрїФ рѕ«рЅдрЅх рІерЇЁрѕЂрЇЇ рѕ░ріљрІХрЅй рѕЇрІерЅ│ рІерЅЃрѕІрЅх рЅхріГріГрѕѕріЏріљрЅхріЋ рѕЏрѕерїІрїѕрїЦ рЅарЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рЅхріЋрЅ░ріЊ рѕўрѕ░рѕерЅх рЇЁрѕЂрЇјрЅйріЋ рѕѕрѕЏрІІрЅђрѕГ ріЦріЊ рѕѕрѕўрѕўрѕхрѕерЅх рѕерїЁрѕЮ рЇЁрѕЂрЇјрЅйріЋ рѕЏрѕ│рїарѕГ ріаріЋрі│рѕГ рїЅрІ│рІ«рЅйріЋ рѕўрѕѕрІерЅх рІѕрІГрѕЮ рїЦрЅЁрѕЇ рѕђрѕ│рЅЦ рѕЏрІЇрїБрЅх ріЋрїЇрїЇрѕГріЋ рІѕрІ░ рЇЁрѕЂрЇЇ рІерѕџрЅђрІГрѕЕ ріарїѕрѕЇрїЇрѕјрЅХрЅйріЋ рІерѕџрѕ░рїА рѕўрЅ░рїЇрЅарѕфрІФ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїі ріљрІЇ рІеріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рЅ┤ріГріќрѕјрїѓ рІўрѕГрЇЇ рЅарїЦріЊрЅх рѕЮрѕГрѕЮріЊ рѕЇрѕЏрЅх ріЦріЋрІ▓рѕўрѕФ рѕЏрѕхрЅ╗рѕЇ ріарѕхрЇѕрѕІрїі рІерІ▓рїѓрЅ│рѕЇ рѕўрѕ░рѕерЅ░ рѕЇрѕЏрЅХрЅй рѕЏрѕЎрѕЈрЅх рѕўрѕ░рѕерЅ│рІі рѕѕрІЇрїЦ рѕѕрѕЏрѕЮрїБрЅх ріЦріЊ рІеріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рІЇрїцрЅХрЅйріЋ рЅарЅ░рїЇрЅБрѕГ рЅарѕхрЇІрЅх рїЦрЅЁрѕЮ рѕІрІГ ріЦріЋрІ▓рІЅрѕЅ рІГрѕерІ│рѕЇ")
  ```

  - Here is a simple example of performing text cleaning on a piece of plaintext using `clean_amharic` function:

  ```python
  from etltk.lang.am import clean_amharic

  # Annotating a Document
  clean = clean_amharic("NLP рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй My email is john.doe@email.com. рѕЏрІўрїІрїђрЅх amharic 125 <html><h1>ріарѕхрЇѕрѕІрїі</h1></html> 456 ріљрІЇ пД┘ёп▒п│пДпд┘ё  Т╝бтГЌ; simplified Chinese: Т▒ЅтГЌ; ­ЪцЌРГЋ­ЪцЊ­Ъцћ")

  # The following example shows how to print all sentences in a document:
  print(clean)
  # output: 'рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй рѕЏрІўрїІрїђрЅх ріарѕхрЇѕрѕІрїі ріљрІЇ'

2. Tokenization - Sentence
    - Here is a simple example of performing sentence tokenization on a piece of plaintext using AmharicDocument:
    
    ```python
    from etltk import AmharicDocument

    # Annotating a Document
    doc = AmharicDocument(sample_text)

    # The following example shows how to print all sentences in a document:
    print(doc.sentences)
    # output: [Sentence("рѕџрІФрІЮрІФ рІЊрѕўрЅ░ рѕЮрѕЁрѕерЅх рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх ріарѕЂріЋ ріФрѕѕрЅарЅх рІЮрЅЁрЅ░ріЏ рІ░рѕерїЃ рІѕрІ░ рѕІрЅђ рІ░рѕерїЃ рѕѕрѕЏрІхрѕерѕх рѕђрїѕрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕріарѕѕрѕЮ рЅ░рІ░рѕФрѕй рѕѕрѕЏрІхрѕерїЇ ріарїѕрѕФрІі ріарЅЁрѕЮріЋ рѕѕрѕЏрѕ│рІ░рїЇ ріЦріЊ рЅ░рїарЅЃрѕџ рѕѕрѕўрѕєріЋ рЅарїІрѕФ ріарЅЦрѕ« рѕўрѕхрѕФрЅ▒ ріЦрїЁрїЇ рїарЅЃрѕџ ріљрІЇ"),
    # Sentence("рЅарѕЏрѕйріЋ ріарѕхрЅ░рѕЮрѕ« ріарѕЏріФріЮріљрЅх рІерЇЁрѕЂрЇЇ ріЊрѕЎріЊрІјрЅй рЅаріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рѕхрѕГріарЅх рѕѕрѕЏрѕ░рѕЇрїаріЋ рІерЇЁрѕЂрЇЇ рІ│рЅ│ріЋ рѕўрѕ░рЅЦрѕ░рЅЦ ріЦріЊ рѕЏрІ░рѕФрїђрЅх"),
    # Sentence("рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅйріЋ рЅарѕўрїарЅђрѕЮ рІерЇЁрѕЂрЇЇ рІ│рЅ│ріЋ рЇЋрѕ«рѕ░рѕх рѕЏрІхрѕерїЇ рЅ░рЅђрІ│рѕџ ріЦріЊ рѕўрѕ░рѕерЅ│рІі рїЅрІ│рІГ ріљрІЇ"),
    # Sentence("рІерѕЏрѕйріЋ рѕѕрѕГріњріЋрїЇ рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй рЅарѕўрїарЅђрѕЮ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅхріЊ рѕўрѕерІ│рЅх рІерЇЁрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅх рІерЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй ріЦріЊ рѕърІ┤рѕјрЅйріЋ рѕЏрІўрїІрїђрЅх ріарѕхрЇѕрѕІрїі ріљрІЇ"),
    # Sentence("рІерЅхрѕГрїЅрѕЮ ріарїѕрѕЇрїЇрѕјрЅх рЅ╗рЅхрЅдрЅх рІерІЇрІГрІГрЅх рѕўрѕѕрІІрІѕрїФ рѕ«рЅдрЅх рІерЇЁрѕЂрЇЇ рѕ░ріљрІХрЅй рѕЇрІерЅ│ рІерЅЃрѕІрЅх рЅхріГріГрѕѕріЏріљрЅхріЋ рѕЏрѕерїІрїѕрїЦ рЅарЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рЅхріЋрЅ░ріЊ рѕўрѕ░рѕерЅх рЇЁрѕЂрЇјрЅйріЋ рѕѕрѕЏрІІрЅђрѕГ ріЦріЊ рѕѕрѕўрѕўрѕхрѕерЅх рѕерїЁрѕЮ рЇЁрѕЂрЇјрЅйріЋ рѕЏрѕ│рїарѕГ ріаріЋрі│рѕГ рїЅрІ│рІ«рЅйріЋ рѕўрѕѕрІерЅх рІѕрІГрѕЮ рїЦрЅЁрѕЇ рѕђрѕ│рЅЦ рѕЏрІЇрїБрЅх ріЋрїЇрїЇрѕГріЋ рІѕрІ░ рЇЁрѕЂрЇЇ рІерѕџрЅђрІГрѕЕ ріарїѕрѕЇрїЇрѕјрЅХрЅйріЋ рІерѕџрѕ░рїА рѕўрЅ░рїЇрЅарѕфрІФ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїі ріљрІЇ"),
    # Sentence("рІеріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рЅ┤ріГріќрѕјрїѓ рІўрѕГрЇЇ рЅарїЦріЊрЅх рѕЮрѕГрѕЮріЊ рѕЇрѕЏрЅх ріЦріЋрІ▓рѕўрѕФ рѕЏрѕхрЅ╗рѕЇ ріарѕхрЇѕрѕІрїі рІерІ▓рїѓрЅ│рѕЇ рѕўрѕ░рѕерЅ░ рѕЇрѕЏрЅХрЅй рѕЏрѕЎрѕЈрЅх рѕўрѕ░рѕерЅ│рІі рѕѕрІЇрїЦ рѕѕрѕЏрѕЮрїБрЅх ріЦріЊ рІеріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рІЇрїцрЅХрЅйріЋ рЅарЅ░рїЇрЅБрѕГ рЅарѕхрЇІрЅх рїЦрЅЁрѕЮ рѕІрІГ ріЦріЋрІ▓рІЅрѕЅ рІГрѕерІ│рѕЇ")]
    ```

    - Here is a simple example of performing sentence tokenization on a piece of plaintext using `sentence_tokenize` function:

    ```python
    from etltk.tokenize.am import sent_tokenize

    # Annotating a Document
    sentences = sent_tokenize("рІерѕЃрїѕрѕфріЏ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅх ріЦріЊ рѕѕрѕўрѕерІ│рЅхрЇБ рІерїйрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅхрЇБ рІерЅІріЋрЅІрІјрЅ╣ріЋ рѕЁрїЇрїІрЅх рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІерѕЃрїѕрѕфріЏ рЅ░рЇѕрїЦрѕ»рІі рЅІріЋрЅІ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй (NLP Tools)рЇБ рѕхрѕЇрЅ░-рЅђрѕўрѕ«рЅй /Algorithms/ ріЦріЊ рѕърІ┤рѕјрЅйріЋ /ML models/ рЅарѕЏрІўрїІрїђрЅх рЅ░рІ░рѕФрѕй рѕЏрІхрѕерїЇ рЅ░рїѕрЅб ріљрІЇрЇб рЅарІџрѕЁрѕЮ рѕўрѕ░рѕерЅх ріарѕЏрѕГріЏрЇБ ріарЇІріЋ рідрѕ«рѕърЇБ рѕХрѕЏрѕіріЏ ріЦріЊ рЅхрїЇрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрѕЏрѕйріЋ рІерѕЏрѕхрЅ░рѕЏрѕГ рѕѓрІ░рЅхріЋ рЅђрѕІрѕЇріЊ рІерЅ░рЅђрѕІрЅ░рЇЇ ріЦріЋрІ▓рѕєріЋ рІФрѕхрЅйрѕІрѕЇрЇАрЇА")

    # The following example shows how to print all sentences in a document:
    print(sentences)
    # output: ['рІерѕђрїѕрѕфріЏ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅх ріЦріЊ рѕѕрѕўрѕерІ│рЅх рІерЇЁрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅх рІерЅІріЋрЅІрІјрЅ╣ріЋ рѕЁрїЇрїІрЅх рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІерѕђрїѕрѕфріЏ рЅ░рЇѕрїЦрѕ»рІі рЅІріЋрЅІ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй ріЦріЊ рѕърІ┤рѕјрЅйріЋ рЅарѕЏрІўрїІрїђрЅх рЅ░рІ░рѕФрѕй рѕЏрІхрѕерїЇ рЅ░рїѕрЅб ріљрІЇ', 'рЅарІџрѕЁрѕЮ рѕўрѕ░рѕерЅх ріарѕЏрѕГріЏ ріарЇІріЋ рідрѕ«рѕъ рѕХрѕЏрѕіріЏ ріЦріЊ рЅхрїЇрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрѕЏрѕйріЋ рІерѕЏрѕхрЅ░рѕЏрѕГ рѕѓрІ░рЅхріЋ рЅђрѕІрѕЇріЊ рІерЅ░рЅђрѕІрЅ░рЇЇ ріЦріЋрІ▓рѕєріЋ рІФрѕхрЅйрѕІрѕЇ']

3. Tokenization - Word
    - Here is a simple example of performing word tokenization on a piece of plaintext using AmharicDocument:
    
    ```python
    from etltk import AmharicDocument

    # Annotating a Document
    doc = AmharicDocument(sample_text)

    # The following example shows how to print all sentences in a document:
    print(doc.words)
    # output: WordList(['рѕџрІФрІЮрІФ', 'рІЊрѕўрЅ░', 'рѕЮрѕЁрѕерЅх', 'рЅаріарїѕрѕГ', 'рІ░рѕерїЃ', 'рІерѕ░рІЇ', 'рѕ░рѕФрѕй', 'ріарѕхрЅ░рІЇрѕјрЅх', 'ріарѕЂріЋ', 'ріФрѕѕрЅарЅх', 'рІЮрЅЁрЅ░ріЏ', 'рІ░рѕерїЃ', 'рІѕрІ░', 'рѕІрЅђ', 'рІ░рѕерїЃ', 'рѕѕрѕЏрІхрѕерѕх', 'рѕђрїѕрѕГріЏ', 'рЅІріЋрЅІрІјрЅйріЋ', 'рѕѕріарѕѕрѕЮ', 'рЅ░рІ░рѕФрѕй', 'рѕѕрѕЏрІхрѕерїЇ', 'ріарїѕрѕФрІі', 'ріарЅЁрѕЮріЋ', 'рѕѕрѕЏрѕ│рІ░рїЇ', 'ріЦріЊ', 'рЅ░рїарЅЃрѕџ', 'рѕѕрѕўрѕєріЋ', 'рЅарїІрѕФ', 'ріарЅЦрѕ«', 'рѕўрѕхрѕФрЅ▒', 'ріЦрїЁрїЇ', 'рїарЅЃрѕџ', 'ріљрІЇрЇАрЇА', 'рЅарѕЏрѕйріЋ', 'ріарѕхрЅ░рѕЮрѕ«', 'ріарѕЏріФріЮріљрЅх', 'рІерЇЁрѕЂрЇЇ', 'ріЊрѕЎріЊрІјрЅй', 'рЅаріарѕГрЅ▓рЇірѕ╗рѕЇ', 'рібріЋрЅ░рѕѕрїђріЋрѕх', 'рѕхрѕГріарЅх', 'рѕѕрѕЏрѕ░рѕЇрїаріЋ', 'рІерЇЁрѕЂрЇЇ', 'рІ│рЅ│ріЋ', 'рѕўрѕ░рЅЦрѕ░рЅЦ', 'ріЦріЊ', 'рѕЏрІ░рѕФрїђрЅх', 'рІеріЊрЅ╣рѕФрѕЇ', 'рѕІріЋрїЅрІїрїЁ', 'рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ', 'рЅ▒рѕјрЅйріЋ', 'рЅарѕўрїарЅђрѕЮ', 'рІерЇЁрѕЂрЇЇ', 'рІ│рЅ│ріЋ', 'рЇЋрѕ«рѕ░рѕх', 'рѕЏрІхрѕерїЇ', 'рЅ░рЅђрІ│рѕџ', 'ріЦріЊ', 'рѕўрѕ░рѕерЅ│рІі', 'рїЅрІ│рІГ', 'ріљрІЇ', 'рІерѕЏрѕйріЋ', 'рѕѕрѕГріњріЋрїЇ', 'рѕхрѕЇрЅ░', 'рЅђрѕўрѕ«рЅй', 'рЅарѕўрїарЅђрѕЮ', 'рЅІріЋрЅІрІјрЅйріЋ', 'рѕўрѕѕрІерЅхріЊ', 'рѕўрѕерІ│рЅх', 'рІерЇЁрѕЂрЇЇ', 'рІГрІўрЅХрЅйріЋ', 'рѕўрѕѕрІерЅх', 'рІерЅІріЋрЅІріЋ', 'рѕўрІІрЅЁрѕГ', 'рѕўрЅ░ріЋрЅ░ріЋ', 'рІерѕџрІФрѕхрЅйрѕЅ', 'рІеріЊрЅ╣рѕФрѕЇ', 'рѕІріЋрїЅрІїрїЁ', 'рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ', 'рЅ▒рѕјрЅй', 'рѕхрѕЇрЅ░', 'рЅђрѕўрѕ«рЅй', 'ріЦріЊ', 'рѕърІ┤рѕјрЅйріЋ', 'рѕЏрІўрїІрїђрЅх', 'ріарѕхрЇѕрѕІрїі', 'ріљрІЇ', 'рІерЅхрѕГрїЅрѕЮ', 'ріарїѕрѕЇрїЇрѕјрЅх', 'рЅ╗рЅхрЅдрЅх', 'рІерІЇрІГрІГрЅх', 'рѕўрѕѕрІІрІѕрїФ', 'рѕ«рЅдрЅх', 'рІерЇЁрѕЂрЇЇ', 'рѕ░ріљрІХрЅй', 'рѕЇрІерЅ│', 'рІерЅЃрѕІрЅх', 'рЅхріГріГрѕѕріЏріљрЅхріЋ', 'рѕЏрѕерїІрїѕрїЦ', 'рЅарЅІріЋрЅІріЋ', 'рѕўрІІрЅЁрѕГ', 'рЅхріЋрЅ░ріЊ', 'рѕўрѕ░рѕерЅх', 'рЇЁрѕЂрЇјрЅйріЋ', 'рѕѕрѕЏрІІрЅђрѕГ', 'ріЦріЊ', 'рѕѕрѕўрѕўрѕхрѕерЅх', 'рѕерїЁрѕЮ', 'рЇЁрѕЂрЇјрЅйріЋ', 'рѕЏрѕ│рїарѕГ', 'ріаріЋрі│рѕГ', 'рїЅрІ│рІ«рЅйріЋ', 'рѕўрѕѕрІерЅх', 'рІѕрІГрѕЮ', 'рїЦрЅЁрѕЇ', 'рѕђрѕ│рЅЦ', 'рѕЏрІЇрїБрЅх', 'ріЋрїЇрїЇрѕГріЋ', 'рІѕрІ░', 'рЇЁрѕЂрЇЇ', 'рІерѕџрЅђрІГрѕЕ', 'ріарїѕрѕЇрїЇрѕјрЅХрЅйріЋ', 'рІерѕџрѕ░рїА', 'рѕўрЅ░рїЇрЅарѕфрІФ', 'рѕЏрѕЇрѕЏрЅх', 'ріарѕхрѕерѕІрїі', 'ріљрІЇ', 'рІеріарѕГрЅ▓рЇірѕ╗рѕЇ', 'рібріЋрЅ░рѕѕрїђріЋрѕх', 'рЅ┤ріГріќрѕјрїѓ', 'рІўрѕГрЇЇ', 'рЅарїЦріЊрЅх', 'рѕЮрѕГрѕЮріЊ', 'рѕЇрѕЏрЅх', 'ріЦріЋрІ▓рѕўрѕФ', 'рѕЏрѕхрЅ╗рѕЇ', 'ріарѕхрЇѕрѕІрїі', 'рІерІ▓рїѓрЅ│рѕЇ', 'рѕўрѕ░рѕерЅ░', 'рѕЇрѕЏрЅХрЅй', 'рѕЏрѕЎрѕЈрЅх', 'рѕўрѕ░рѕерЅ│рІі', 'рѕѕрІЇрїЦ', 'рѕѕрѕЏрѕЮрїБрЅх', 'ріЦріЊ', 'рІеріарѕГрЅ▓рЇірѕ╗рѕЇ', 'рібріЋрЅ░рѕѕрїђріЋрѕх', 'рІЇрїцрЅХрЅйріЋ', 'рЅарЅ░рїЇрЅБрѕГ', 'рЅарѕхрЇІрЅх', 'рїЦрЅЁрѕЮ', 'рѕІрІГ', 'ріЦріЋрІ▓рІЅрѕЅ', 'рІГрѕерІ│рѕЇ'])
    ```

    - Here is a simple example of performing sentence tokenization on a piece of plaintext using `word_tokenize` function:
    
    ```python
    from etltk.tokenize.am import word_tokenize
      
    # Annotating a Document
    words = word_tokenize("рІерѕЃрїѕрѕфріЏ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅх ріЦріЊ рѕѕрѕўрѕерІ│рЅхрЇБ рІерїйрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅхрЇБ рІерЅІріЋрЅІрІјрЅ╣ріЋ рѕЁрїЇрїІрЅх рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІерѕЃрїѕрѕфріЏ рЅ░рЇѕрїЦрѕ»рІі рЅІріЋрЅІ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй (NLP Tools)рЇБ рѕхрѕЇрЅ░-рЅђрѕўрѕ«рЅй /Algorithms/ ріЦріЊ рѕърІ┤рѕјрЅйріЋ /ML models/ рЅарѕЏрІўрїІрїђрЅх рЅ░рІ░рѕФрѕй рѕЏрІхрѕерїЇ рЅ░рїѕрЅб ріљрІЇрЇб рЅарІџрѕЁрѕЮ рѕўрѕ░рѕерЅх ріарѕЏрѕГріЏрЇБ ріарЇІріЋ рідрѕ«рѕърЇБ рѕХрѕЏрѕіріЏ ріЦріЊ рЅхрїЇрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрѕЏрѕйріЋ рІерѕЏрѕхрЅ░рѕЏрѕГ рѕѓрІ░рЅхріЋ рЅђрѕІрѕЇріЊ рІерЅ░рЅђрѕІрЅ░рЇЇ ріЦріЋрІ▓рѕєріЋ рІФрѕхрЅйрѕІрѕЇрЇАрЇА")

    # The following example shows how to print all sentences in a document:
    print(words)
    # output: ['рІерѕђрїѕрѕфріЏ', 'рЅІріЋрЅІрІјрЅйріЋ', 'рѕўрѕѕрІерЅх', 'ріЦріЊ', 'рѕѕрѕўрѕерІ│рЅх', 'рІерЇЁрѕЂрЇЇ', 'рІГрІўрЅХрЅйріЋ', 'рѕўрѕѕрІерЅх', 'рІерЅІріЋрЅІрІјрЅ╣ріЋ', 'рѕЁрїЇрїІрЅх', 'рѕўрЅ░ріЋрЅ░ріЋ', 'рІерѕџрІФрѕхрЅйрѕЅ', 'рІерѕђрїѕрѕфріЏ', 'рЅ░рЇѕрїЦрѕ»рІі', 'рЅІріЋрЅІ', 'рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ', 'рЅ▒рѕјрЅй', 'рѕхрѕЇрЅ░', 'рЅђрѕўрѕ«рЅй', 'ріЦріЊ', 'рѕърІ┤рѕјрЅйріЋ', 'рЅарѕЏрІўрїІрїђрЅх', 'рЅ░рІ░рѕФрѕй', 'рѕЏрІхрѕерїЇ', 'рЅ░рїѕрЅб', 'ріљрІЇ', 'рЅарІџрѕЁрѕЮ', 'рѕўрѕ░рѕерЅх', 'ріарѕЏрѕГріЏ', 'ріарЇІріЋ', 'рідрѕ«рѕъ', 'рѕХрѕЏрѕіріЏ', 'ріЦріЊ', 'рЅхрїЇрѕГріЏ', 'рЅІріЋрЅІрІјрЅйріЋ', 'рѕѕрѕЏрѕйріЋ', 'рІерѕЏрѕхрЅ░рѕЏрѕГ', 'рѕѓрІ░рЅхріЋ', 'рЅђрѕІрѕЇріЊ', 'рІерЅ░рЅђрѕІрЅ░рЇЇ', 'ріЦріЋрІ▓рѕєріЋ', 'рІФрѕхрЅйрѕІрѕЇ']

4. Normalization
    1. Character Level Normalization such as "`рїИ`рѕђрІГ" and "`рЇђ`рѕљрІГ"
    2. Labialized Character Normalzation such as "рѕърѕЇ`рЅ▒рІІ`рѕЇ" to "рѕърѕЇ`рЅи`рѕЇ"
    3. Short Form Expansion such as "`ріа.ріа`" to "`ріарІ▓рѕх ріарЅарЅБ`"
    4. Punctuation Normalization such as `::` to `рЇб`

    - Here is a simple example of performing normalization on a piece of plaintext using AmharicDocument:

    ```python
    from etltk.lang.am import normalize

    # normalizing a Document
    normalized = normalize("DHL рІерІЋрѕѕрЅ▒ My email is john.doe@email.com. ріЦріЊрЅђрѕГрЅБрѕѕріЋрЇб amharic 125 <html><h1>Title</h1^X^X></html> 456 processor 18 пД┘ёп▒п│пДпд┘ё  Т╝бтГЌ; simplified Chinese: Т▒ЅтГЌ; ­ЪцЌРГЋ­ЪцЊ­Ъцћ")

    # The following example shows how to print all normalized in a document:
    print(normalized)
    # output: рІерІЋрѕѕрЅ▒     ріЦріЊрЅђрѕГрЅБрѕѕріЋ
    ```

    - Here is a simple example of performing normalization on a piece of plaintext using `normalize_char`, `normalize_punct`, `normalize_labialized`, `normalize_shortened` function:
    
    ```python
    from etltk.lang.am.normalizer import (
      normalize_char, 
      normalize_labialized, 
      normalize_punct, 
      normalize_shortened)

    # normalization
    char_norm = normalize_char(")

    # The following example shows how to print all sentences in a document:
    print(char_norm)
    # output:

