{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction**\n",
    "\n",
    "The ultimate goal is to detect fish point patterns for unique identification via topological methods.\n",
    "There are some papers on it that it is possible to distinguish certain types of fish with their respective dot patters,\n",
    "but it remains for example unclear for how many fishes their methods can actually work, since lab envirnonments have at maximum 30 fishes.\n",
    "At the same time, a meaningful encoding is difficult to achieve.\n",
    "We therefore try to use persistent homology methods to \"save\" the dot patterns in a machine learning friendly way, which then can be used to identify different fishes.\n",
    "As a first step, we want to use PersLay to find out how many different classes of random pattern can be distinguished at all with certain architectures."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For that purpose, we create an artificial dataset of 2d points.\n",
    "We fix the number of classes, and the number of \"variations\" of that point-image (noise added) to account for variations\n",
    "in previous image processing methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n",
    "def create_test_dataset(number_of_different_classes=100, images_per_class=10, dimensions=2, number_of_points=10,\n",
    "                        noise=0.1, overwrite=False):\n",
    "    \"\"\"Create random points in a 2D-grid from -1 to 1\n",
    "    :param number_of_different_classes: THe number of wished classes\n",
    "    :type number_of_different_classes: int\n",
    "    :param images_per_class: The number of noised images that one class should contain\n",
    "    :type images_per_class: int\n",
    "    :param dimensions: dimension of the dataset\n",
    "    :type dimensions: int\n",
    "    :param number_of_points: Number of datapoints for each image\n",
    "    :type number_of_points: int\n",
    "    :param noise: The added noise to the data\n",
    "    :type noise: float\n",
    "    :param overwrite: Whether a new data file will be written\n",
    "    :type overwrite: bool\n",
    "    :return: None\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    if overwrite or not pathlib.Path('project_data_training.pkl').exists():\n",
    "        np.random.seed(0)\n",
    "        json_file = dict()\n",
    "        json_file[\"info\"] = \"Dataset for random point patterns\"\n",
    "        data = dict()\n",
    "        noised_images = dict()\n",
    "        counter = 0\n",
    "        # Create the classes\n",
    "        for i in range(number_of_different_classes):\n",
    "            samples = np.array(np.random.random(number_of_points * dimensions) * 20 - 10)\n",
    "            reshaped_samples = np.reshape(samples, (number_of_points, dimensions))\n",
    "            # Create the noised images\n",
    "            for j in range(images_per_class):\n",
    "                noised_images[counter] = dict()\n",
    "                sample_noise = np.array(np.random.random(number_of_points * dimensions) * noise - (.5 * noise))\n",
    "                reshaped_sample_noise = np.reshape(sample_noise, (number_of_points, dimensions))\n",
    "                noised_images[counter][\"image_id\"] = counter\n",
    "                noised_images[counter][\"class\"] = i\n",
    "                noised_images[counter][\"image\"] = reshaped_samples + reshaped_sample_noise\n",
    "                counter += 1\n",
    "            data[\"noised_images\"] = noised_images\n",
    "        json_file[\"data\"] = data\n",
    "        # Only when debugging\n",
    "        # print(json_file)\n",
    "        with open('project_data_training.pkl', 'wb') as f:\n",
    "            pickle.dump(json_file, f)\n",
    "        print(\"New dataset created\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Furthermore, we need a method to calculate the persistence diagrams.\n",
    "Here we can see that our appraoch seems meaningful,\n",
    "since we already get very different persistence diagrams for different classes,\n",
    "but similar ones within the variation group. To turn on drawing, set the \"draw\" variable to true in the run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def write_dataset_of_persistence_diagrams(max_simplices_dim=2, overwrite=False, draw=False,\n",
    "                                          filter_by_immediate_deaths=False, filter_by_max_value=False,\n",
    "                                          filter_to_one_dimensional=False):\n",
    "    \"\"\"Saves a dataset of persistence diagrams based on the fed data.\n",
    "    :param max_simplices_dim: Max. dimension of simplicies\n",
    "    :type max_simplices_dim: int\n",
    "    :param overwrite: Whether a new dataset of persistence diagrams is created\n",
    "    :type overwrite: bool\n",
    "    :param draw: Whether plots for EACH persistence diagram is being showed\n",
    "    :type draw: bool\n",
    "    :param filter_by_immediate_deaths: Whether to filter out persistence pairs on the diagonal\n",
    "    :type filter_by_immediate_deaths: bool\n",
    "    :param filter_by_max_value: Whether to filter out persistence pairs with infinity as death values\n",
    "    :type filter_by_max_value: bool\n",
    "    :param filter_to_one_dimensional: Whether to filter out persistence pairs which do not have the dimension 1.\n",
    "    :type filter_to_one_dimensional: bool\n",
    "    :return: None\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    if not pathlib.Path('project_data_training.pkl').exists():\n",
    "        raise FileNotFoundError(\"We cannot find the project data file!\")\n",
    "    if overwrite or not pathlib.Path('pers_diagram_training.pkl').exists():\n",
    "        # Load data\n",
    "        with open(\"project_data_training.pkl\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        persistence_barcodes = dict()\n",
    "        current_class = -1\n",
    "        current_variation = 0\n",
    "        for key_id, noised_image in data[\"data\"][\"noised_images\"].items():\n",
    "\n",
    "            persistence_barcode = dict()\n",
    "            training_data = noised_image[\"image\"]\n",
    "            persistence_barcode[\"class\"] = noised_image[\"class\"]\n",
    "            persistence_barcode[\"barcode_id\"] = noised_image[\"image_id\"]\n",
    "            # Saving class and variation for display\n",
    "            if current_class == persistence_barcode['class']:\n",
    "                current_variation = current_variation +1\n",
    "            else:\n",
    "                current_class = persistence_barcode[\"class\"]\n",
    "                current_variation = 0\n",
    "            # Calculate Filtered Complex on the training data\n",
    "            filtered_complex = FilteredComplexes(training_data)\n",
    "            bounday_matrix, dimensional_array, distance_list = filtered_complex.fit(max_simplices_dim=max_simplices_dim,\n",
    "                                                                                    k_nearest_neighours=5)\n",
    "            matrix_reductor = MatrixReduction(bounday_matrix, dimensional_array, distance_list=distance_list)\n",
    "            matrix_reductor.reduce_boundary_matrix()\n",
    "            if draw:\n",
    "                matrix_reductor.draw_persistence_diagram(plot_inf=False, title=f\"Persistence diagram of class {persistence_barcode['class']}, variation {current_variation}\")\n",
    "\n",
    "            barcode = matrix_reductor.get_barcode_of_reduced_matrix()\n",
    "            barcode[barcode == np.inf] = -1\n",
    "            max_value = np.max(barcode[\"death_value\"]) + 100\n",
    "            barcode[barcode == -1] = max_value\n",
    "            if filter_by_max_value:\n",
    "                barcode = barcode[barcode[\"death_value\"] != max_value]\n",
    "                print(f\"Barcode after max filtering: {len(barcode)}\")\n",
    "            if filter_by_immediate_deaths:\n",
    "                barcode = barcode[barcode[\"birth_value\"] != barcode[\"death_value\"]]\n",
    "                print(f\"Barcode after immediate deaths filtering: {len(barcode)}\")\n",
    "            if filter_to_one_dimensional:\n",
    "                barcode = barcode[barcode[\"dimension\"] == 1]\n",
    "                print(f\"Barcode after filtering for one dimensional simplices: {len(barcode)}\")\n",
    "            barcode_numpy = barcode[[\"birth_value\", \"death_value\"]].to_numpy()\n",
    "            persistence_barcode[\"barcode\"] = barcode_numpy\n",
    "\n",
    "            persistence_barcodes[key_id] = persistence_barcode\n",
    "\n",
    "        with open('pers_diagram_training.pkl', 'wb') as f:\n",
    "            pickle.dump(persistence_barcodes, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to train on the persistence diagrams, we need to create a custom dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomPersistenceDiagramDataset(Dataset):\n",
    "    \"\"\"Custom Dataset inherit after the conventions of pytorchin order to use it in a DataLoader.\"\"\"\n",
    "    def __init__(self, project_data_file=\"pers_diagram_training.pkl\"):\n",
    "        with open(project_data_file, 'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            self.data = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return np.array(item[\"barcode\"], dtype=float), item[\"class\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the last step, we train the neural network based on the PersLay on the persistence diagrams.\n",
    "That one does not work yet though...\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "def run_project_perslay(rho_output_dim_q=25):\n",
    "    \"\"\"Runs the training loop for the perslay neural network. DIUSCLAIMER: DOES NOT WORK! Plots a loss plot.\n",
    "    :param rho_output_dim_q: Output dimention of the perslay\n",
    "    :type rho_output_dim_q: int\n",
    "    :return: None\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # for debugging\n",
    "    # device = 'cpu'\n",
    "    dataset = CustomPersistenceDiagramDataset()\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "    perslay = PersLay(layer_type=\"persistence_diagram\", rho_output_dim_q=rho_output_dim_q)\n",
    "    perslay.to(device)\n",
    "    # Loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    criterion.requires_grad = True\n",
    "    optimizer = optim.SGD(perslay.parameters(), lr=0.1)\n",
    "    lambda1 = lambda epoch: max(0.99 ** epoch, 0.00001 / 0.1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "    loss_stats = []\n",
    "    epoch_stats = []\n",
    "    i = 0\n",
    "    for t in range(1000):\n",
    "        # Training\n",
    "        for idx, (train_data, train_labels) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Transfer to GPU\n",
    "            train_data = train_data.to(device)\n",
    "            train_labels = train_labels.to(device)\n",
    "            # Forward pass: compute predicted y\n",
    "            pred_prob = perslay.forward(torch.squeeze(train_data, 0).float())\n",
    "            y_pred = np.argmax(pred_prob.cpu().detach().numpy(), axis=0)\n",
    "\n",
    "            # Compute and print loss\n",
    "            loss = criterion(torch.FloatTensor([y_pred]), train_labels.type(torch.FloatTensor))\n",
    "            loss_stats.append(loss.cpu().detach().numpy())\n",
    "            epoch_stats.append(i)\n",
    "            i = i + 1\n",
    "            if t % 10 == 0:\n",
    "                print(f\"Epoch {t}\")\n",
    "                print(f\"Predicted: {y_pred}\")\n",
    "                print(f\"Actual: {train_labels.cpu().detach().numpy()}\")\n",
    "                print(\"loss :\", loss.cpu().detach().numpy())\n",
    "            loss.requires_grad = True\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(\"Training finished\")\n",
    "    df = pandas.DataFrame(data=np.array([epoch_stats, loss_stats]).transpose(), columns=[\"step\", \"loss\"])\n",
    "    fig = px.line(df, x=\"step\", y=\"loss\")\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us run all the code!\n",
    "The overwrite attribute allows to overwrite the existing saved dataset (we save some time skipping the creation every time).\n",
    "draw=True will let the persistence diagrams calculated pop up visually."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from persistent_homology import FilteredComplexes, \\\n",
    "    MatrixReduction, PersLay\n",
    "overwrite = True\n",
    "draw = True\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "classes = 10\n",
    "max_simplices_dim = 2\n",
    "create_test_dataset(number_of_different_classes=classes, images_per_class=3, number_of_points=15, noise=0.5,\n",
    "                    overwrite=overwrite)\n",
    "write_dataset_of_persistence_diagrams(max_simplices_dim=max_simplices_dim, overwrite=overwrite, draw=draw,\n",
    "                                      filter_by_immediate_deaths=True, filter_by_max_value=True,\n",
    "                                      filter_to_one_dimensional=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset created\n",
      "All radii: [ 0.          5.15939238  5.28524743  5.92925472  6.21834547  6.63292745\n",
      "  6.68670184  6.79202428  7.26493206  7.30768955  7.38248501  7.67295683\n",
      "  7.84440952  7.84440952  7.91414901  7.95237288  8.43318638  8.44768168\n",
      "  8.52949001  8.826579    8.86137315  8.87853547  9.02149458  9.08700558\n",
      "  9.27444436  9.33633399  9.86117849  9.95909629 10.11821695 10.14811464\n",
      " 10.17981538 10.38387521 10.46600496 10.79087845 10.83710136 10.9191797\n",
      " 10.97279305 11.34789002 11.75777447 12.00248115 12.04346156 12.41223949\n",
      " 12.6086717  12.60948469 12.97517012 13.0099183  13.1206103  13.2475455\n",
      " 13.70351386 14.06077411 14.69176301 14.72823518 15.23142916 15.23587804\n",
      " 15.70329139 15.74904972 16.59660384 16.71419428 16.9003651  17.00316465\n",
      " 17.64825537 17.70230087 17.94834356 18.94369669 19.24011284 19.35474261\n",
      " 19.87910478 20.86743804 21.11447571 22.96159979]\n",
      "Calculating boundary_matrix: This may take a while...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_project_perslay(rho_output_dim_q=classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}